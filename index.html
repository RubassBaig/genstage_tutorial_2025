
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Introduction Draft Viewer</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background: #f5f5f5;
        }

        .controls {
            background: white;
            padding: 15px;
            border-radius: 8px;
            margin-bottom: 20px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            display: flex;
            gap: 10px;
            align-items: center;
        }

        button {
            background: #4CAF50;
            color: white;
            border: none;
            padding: 10px 20px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background 0.3s;
        }

        button:hover {
            background: #45a049;
        }

        button.active {
            background: #2196F3;
        }

        .content-container {
            display: flex;
            gap: 20px;
        }

        .single-view {
            background: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            width: 100%;
        }

        .side-by-side {
            display: flex;
            gap: 20px;
            width: 100%;
        }

        .side-panel {
            background: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            flex: 1;
            overflow-y: auto;
            max-height: 80vh;
        }

        .version-label {
            background: #e0e0e0;
            padding: 5px 10px;
            border-radius: 4px;
            font-size: 12px;
            font-weight: bold;
            margin-bottom: 20px;
            display: inline-block;
        }

        .version-label.original {
            background: #9c27b0;
            color: white;
        }

        .version-label.bobby {
            background: #4CAF50;
            color: white;
        }

        /* Article styling */
        .section-title {
            font-size: 14px;
            font-weight: bold;
            color: #666;
            text-transform: uppercase;
            letter-spacing: 1px;
            margin-bottom: 10px;
        }

        .aside {
            background: #f5f5f5;
            border-left: 3px solid #e74c3c;
            padding: 20px;
            margin: 20px 0;
            position: relative;
        }

        .aside-header {
            font-size: 12px;
            font-weight: bold;
            color: #e74c3c;
            text-transform: uppercase;
            letter-spacing: 1px;
            margin-bottom: 0;
            cursor: pointer;
            user-select: none;
            display: flex;
            align-items: center;
            justify-content: space-between;
        }

        .aside-header .aside-label {
            color: #e74c3c;
            margin-right: 10px;
        }

        .aside-header .aside-title {
            color: #333;
            text-transform: none;
            letter-spacing: normal;
            font-size: 14px;
            flex: 1;
        }

        .aside-header:after {
            content: 'â–¼';
            font-size: 10px;
            transition: transform 0.3s ease;
            margin-left: 10px;
        }

        .aside.collapsed .aside-header:after {
            transform: rotate(-90deg);
        }

        .aside-content {
            margin-top: 15px;
            transition: max-height 0.3s ease, opacity 0.3s ease;
            overflow: hidden;
            max-height: 2000px;
            opacity: 1;
        }

        .aside.collapsed .aside-content {
            max-height: 0;
            opacity: 0;
            margin-top: 0;
        }

        h1 {
            color: #2c3e50;
            border-bottom: 2px solid #eee;
            padding-bottom: 10px;
            margin-bottom: 20px;
            margin-top: 0;
        }

        h2, h3 {
            color: #34495e;
            margin-top: 30px;
        }

        p {
            margin-bottom: 15px;
            line-height: 1.8;
        }

        ul, ol {
            margin-bottom: 15px;
            padding-left: 30px;
        }

        li {
            margin-bottom: 8px;
        }

        pre {
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 4px;
            padding: 15px;
            overflow-x: auto;
            font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
            font-size: 14px;
            line-height: 1.4;
        }

        code {
            background: #f8f9fa;
            padding: 2px 4px;
            border-radius: 3px;
            font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
            font-size: 90%;
        }

        pre code {
            background: none;
            padding: 0;
        }

        .hidden {
            display: none;
        }
    </style>
</head>
<body>
    <div class="controls">
        <button id="originalBtn" onclick="showVersion('original')">2015 ORIGINAL</button>
        <button id="bobbyBtn" onclick="showVersion('bobby')">BOBBY Version</button>
        <button id="sideBySideBtn" onclick="toggleSideBySide()">Side by Side</button>
    </div>

    <div id="singleView" class="single-view">
        <div class="version-label original">2015 ORIGINAL</div>
        <div id="singleContent"></div>
    </div>

    <div id="sideBySideView" class="side-by-side hidden">
        <div class="side-panel">
            <div class="version-label original">2015 ORIGINAL</div>
            <div id="originalContent"></div>
        </div>
        <div class="side-panel">
            <div class="version-label bobby">BOBBY VERSION</div>
            <div id="bobbyContent"></div>
        </div>
    </div>

    <script>
        const originalVersion = `
<div class="section-title">2015 ORIGINAL</div>
<h1>GenStage Tutorial</h1>

<h2>Introduction</h2>
<p>So what is GenStage? From the official documentation, it is a "specification and computational flow for Elixir", but what does that mean to us?<br>
There is a lot to something that can be described as that vague, and here we'll take a dive in and build something on top of it to understand its goals.<br>
We could go into the technical and theoretical implications of this, but instead lets try a pragmatic approach to really just get it to work.</p>

<p>First, Let's imagine we have a server that constantly emits numbers.<br>
It starts at the state of the number we give it, then counts up in one from there onward.<br>
This is what we would call our producer.<br>
Each time it emits a number, this is an event, and we want to handle it with a consumer.<br>
A consumer simply takes what a producer emits and does something to it.<br>
In our case, we will display the count.<br>
There is a lot more to GenStage at a technical and applied level, but we will build up on the specifics and definitions further in later lessons, for now we just want a running example we can build up on.</p>

<h2>Getting Started: A Sample GenStage Project</h2>
<p>We'll begin by generating a simple project that has a supervision tree:</p>

<pre><code>$ mix new genstage_example --sup
$ cd genstage_example</code></pre>

<p>Let's set up some basic things for the future of our application.<br>
Since GenStage is generally used as a transformation pipeline, lets imagine we have a background worker of some sort.<br>
This worker will need to persist whatever it changes, so we should get a database set up, but we can worry about that in a later lesson.<br>
To start, all we need to do is add <code>gen_stage</code> to our deps in <code>mix.deps</code>.</p>

<pre><code>. . .
  defp deps do
    [
      {:gen_stage, "~> 0.7"},
    ]
  end
. . .</code></pre>

<p>Now, we should fetch our dependencies and compile before we start setup:</p>

<pre><code>$ mix do deps.get, compile</code></pre>

<p>Lets build a producer, our simple beginning building block to help us utilize this new tool!</p>

<h2>Building A Producer</h2>
<p>To get started what we want to do is create a producer that emits a constant stream of events for our consumer to handle.<br>
This is quite simple with the rudimentary example of a counter.<br>
Let's create a namespaced directory under <code>lib</code> and then go from there, this way our module naming matches our names of the modules themselves:</p>

<pre><code>$ mkdir lib/genstage_example
$ touch lib/genstage_example/producer.ex</code></pre>

<p>Now we can add the code:</p>

<pre><code>defmodule GenstageExample.Producer do
  alias Experimental.GenStage
  use GenStage

  def start_link do
    GenStage.start_link(__MODULE__, 0, name: __MODULE__)
                                       # naming allows us to handle failure
  end

  def init(counter) do
    {:producer, counter}
  end

  def handle_demand(demand, state) do
                    # the default demand is 1000
    events = Enum.to_list(state..state + demand - 1)
             # [0 .. 999]
             # is a filled list, so its going to be considered emitting true events immediately
    {:noreply, events, (state + demand)}
  end
end</code></pre>

<p>Let's break this down line by line.<br>
To begin with, we have our initial declarations:</p>

<pre><code>. . .
defmodule GenstageExample.Producer do
  alias Experimental.GenStage
  use GenStage
. . .</code></pre>

<p>What this does is a couple simple things.<br>
First, we declare our module, and soon after we alias <code>Experimental.GenStage</code>.<br>
This is simply because we will be calling it more than once and makes it more convenient.<br>
The <code>use GenStage</code> line is much akin to <code>use GenServer</code>.<br>
This line allows us to import the default behaviour and functions to save us from a large amount of boilerplate.</p>

<p>If we go further, we see the first two primary functions for startup:</p>

<pre><code>. . .
  def start_link do
    GenStage.start_link(__MODULE__, :the_state_doesnt_matter)
  end

  def init(counter) do
    {:producer, counter}
  end
. . .</code></pre>

<p>These two functions offer a very simple start.<br>
First, we have our standard <code>start_link/0</code> function.<br>
Inside here, we use<code>GenStage.start_link/</code> beginning with our argument <code>__MODULE__</code>, which will give it the name of our current module.<br>
Next, we set a state, which is arbitrary in this case, and can be any value.<br>
The <code>__MODULE__</code> argument is used for name registration like any other module.<br>
The second argument is the arguments, which in this case are meaningless as we do not care about it.<br>
In <code>init/1</code> we simply set the counter as our state, and label ourselves as a producer.</p>

<p>Finally, we have where the real meat of our code's functionality is:</p>

<pre><code>. . .
  def handle_demand(demand, state) do
    events = Enum.to_list(state..state + demand - 1)
    {:noreply, events, (state + demand)}
  end
. . .</code></pre>

<p><code>handle_demand/2</code> must be implemented by all producer type modules that utilize GenStage.<br>
In this case, we are simply sending out an incrementing counter.<br>
This might not make a ton of sense until we build our consumer, so lets move on to that now.</p>

<h2>Building A Consumer</h2>
<p>The consumer will handle the events that are broadcasted out by our producer.<br>
For now, we wont worry about things like broadcast strategies, or what the internals are truly doing.<br>
We'll start by showing all the code and then break it down.</p>

<pre><code>defmodule GenstageExample.Consumer do
  alias Experimental.GenStage
  use GenStage

  def start_link do
    GenStage.start_link(__MODULE__, :state_doesnt_matter)
  end

  def init(state) do
    {:consumer, state, subscribe_to: [GenstageExample.Producer]}
  end

  def handle_events(events, _from, state) do
    for event <- events do
      IO.inspect {self(), event, state}
    end
    {:noreply, [], state}
  end
end</code></pre>

<p>To start, let's look at the beginning functions just like last time:</p>

<pre><code>defmodule GenstageExample.Consumer do
  alias Experimental.GenStage
  use GenStage

  def start_link do
    GenStage.start_link(__MODULE__, :state_doesnt_matter)
  end

  def init(state) do
    {:consumer, state, subscribe_to: [GenstageExample.Producer]}
  end
. . .</code></pre>

<p>To begin, much like in our producer, we set up our <code>start_link/0</code> and <code>init/1</code> functions.<br>
In <code>start_link</code> we simple register the module name like last time, and set a state.<br>
The state is arbitrary for the consumer, and can be literally whatever we please, in this case <code>:state_doesnt_matter</code>.</p>

<p>In <code>init/1</code> we simply take the state and set up our expected tuple.<br>
It expected use to register our <code>:consumer</code> atom first, then the state given.<br>
Our <code>subscribe_to</code> clause is optional.<br>
What this does is subscribe us to our producer module.<br>
The reason for this is if something crashes, it will simply attempt to re-subscribe and then resume receiving emitted events.</p>

<pre><code>. . .
  def handle_events(events, _from, state) do
    for event <- events do
      IO.inspect {self(), event, state}
    end
    {:noreply, [], state}
  end
. . .</code></pre>

<p>This is the meat of our consumer, <code>handle_events/3</code>.<br>
<code>handle_events/3</code> must be implemented by any <code>consumer</code> or <code>producer_consumer</code> type of GenStage module.<br>
What this does for us is quite simple.<br>
We take a list of events, and iterate through these.<br>
From there, we inspect the <code>pid</code> of our consumer, the event (in this case the current count), and the state.<br>
After that, we don't reply because we are a consumer and do not handle anything, and we don't emit events to the second argument is empty, then we simply pass on the state.</p>

<h2>Wiring It Together</h2>
<p>To get all of this to work we only have to make one simple change.<br>
Open up <code>lib/genstage_example.ex</code> and we can add them as workers and they will automatically start with our application:</p>

<pre><code>. . .
    children = [
      worker(GenstageExample.Producer, []),
      worker(GenstageExample.Consumer, []),
    ]
. . .</code></pre>

<p>With this, if things are all correct, we can run IEx and we should see everything working:</p>

<pre><code>iex(1)> {#PID<0.205.0>, 0, :state_doesnt_matter}
{#PID<0.205.0>, 1, :state_doesnt_matter}
{#PID<0.205.0>, 2, :state_doesnt_matter}
{#PID<0.205.0>, 3, :state_doesnt_matter}
{#PID<0.205.0>, 4, :state_doesnt_matter}
{#PID<0.205.0>, 5, :state_doesnt_matter}
{#PID<0.205.0>, 6, :state_doesnt_matter}
{#PID<0.205.0>, 7, :state_doesnt_matter}
{#PID<0.205.0>, 8, :state_doesnt_matter}
{#PID<0.205.0>, 9, :state_doesnt_matter}
{#PID<0.205.0>, 10, :state_doesnt_matter}
{#PID<0.205.0>, 11, :state_doesnt_matter}
{#PID<0.205.0>, 12, :state_doesnt_matter}
. . .</code></pre>

<h2>Tinkering: For Science and Understanding</h2>
<p>From here, we have a working flow.<br>
There is a producer emitting our counter, and our consumber is displaying all of this and continuing the flow.<br>
Now, what if we wanted multiple consumers?<br>
Right now, if we examine the <code>IO.inspect/1</code> output, we see that every single event is handled by a single PID.<br>
This isn't very Elixir-y.<br>
We have massive concurrency built-in, we should probably leverage that as much as possible.<br>
Let's make some adjustments so that we can have multiple workers by modifying <code>lib/genstage_example.ex</code></p>

<pre><code>. . .
    children = [
      worker(GenstageExample.Producer, []),
      worker(GenstageExample.Consumer, [], id: 1),
      worker(GenstageExample.Consumer, [], id: 2),
    ]
. . .</code></pre>

<p>Now, let's fire up IEx again:</p>

<pre><code>$ iex -S mix
iex(1)> {#PID<0.205.0>, 0, :state_doesnt_matter}
{#PID<0.205.0>, 1, :state_doesnt_matter}
{#PID<0.205.0>, 2, :state_doesnt_matter}
{#PID<0.207.0>, 3, :state_doesnt_matter}
. . .</code></pre>

<p>As you can see, we have multiple PIDs now, simply by adding a line of code and giving our consumers IDs.<br>
But we can take this even further:</p>

<pre><code>. . .
    children = [
      worker(GenstageExample.Producer, []),
    ]
    consumers = for id <- 1..(System.schedulers_online * 12) do
                              # helper to get the number of cores on machine
                  worker(GenstageExample.Consumer, [], id: id)
                end

    opts = [strategy: :one_for_one, name: GenstageExample.Supervisor]
    Supervisor.start_link(children ++ consumers, opts)
. . .</code></pre>

<p>What we are doing here is quite simple.<br>
First, we get the number of core on the machine with <code>System.schedulers_online/0</code>, and from there we simply create a worker just like we had.<br>
Now we have 12 workers per core. This is much more effective.</p>

<pre><code>. . .
{#PID<0.229.0>, 63697, :state_doesnt_matter}
{#PID<0.224.0>, 53190, :state_doesnt_matter}
{#PID<0.223.0>, 72687, :state_doesnt_matter}
{#PID<0.238.0>, 69688, :state_doesnt_matter}
{#PID<0.196.0>, 62696, :state_doesnt_matter}
{#PID<0.212.0>, 52713, :state_doesnt_matter}
{#PID<0.233.0>, 72175, :state_doesnt_matter}
{#PID<0.214.0>, 51712, :state_doesnt_matter}
{#PID<0.227.0>, 66190, :state_doesnt_matter}
{#PID<0.234.0>, 58694, :state_doesnt_matter}
{#PID<0.211.0>, 55694, :state_doesnt_matter}
{#PID<0.240.0>, 64698, :state_doesnt_matter}
{#PID<0.193.0>, 50692, :state_doesnt_matter}
{#PID<0.207.0>, 56683, :state_doesnt_matter}
{#PID<0.213.0>, 71684, :state_doesnt_matter}
{#PID<0.235.0>, 53712, :state_doesnt_matter}
{#PID<0.208.0>, 51197, :state_doesnt_matter}
{#PID<0.200.0>, 61689, :state_doesnt_matter}
. . .</code></pre>

<p>Though we lack any ordering like we would have with a single core, but every increment is being hit and processed.</p>

<p>We can take this a step further and change our broadcasting strategy from the default in our producer:</p>

<pre><code>. . .
  def init(counter) do
    {:producer, counter, dispatcher: GenStage.BroadcastDispatcher}
  end
. . .</code></pre>

<p>What this does is it accumulates demand from all consumers before broadcasting its events to all of them.<br>
If we fire up IEx we can see the implication:</p>

<pre><code>. . .
{#PID<0.200.0>, 1689, :state_doesnt_matter}
{#PID<0.230.0>, 1690, :state_doesnt_matter}
{#PID<0.196.0>, 1679, :state_doesnt_matter}
{#PID<0.215.0>, 1683, :state_doesnt_matter}
{#PID<0.237.0>, 1687, :state_doesnt_matter}
{#PID<0.205.0>, 1682, :state_doesnt_matter}
{#PID<0.206.0>, 1695, :state_doesnt_matter}
{#PID<0.216.0>, 1682, :state_doesnt_matter}
{#PID<0.217.0>, 1689, :state_doesnt_matter}
{#PID<0.233.0>, 1681, :state_doesnt_matter}
{#PID<0.223.0>, 1689, :state_doesnt_matter}
{#PID<0.193.0>, 1194, :state_doesnt_matter}
. . .</code></pre>

<p>Note that some numbers are showing twice now, this is why.</p>

<h2>Setting Up Postgres to Extend Our Producer</h2>
<p>To go further we'll need to bring in a database to store our progress and status.<br>
This is quite simple using Ecto.<br>
To get started let's add it and the Postgresql adapter to <code>mix.exs</code>:</p>

<pre><code>. . .
  defp deps do
    [
     {:gen_stage, "~> 0.7"},
     {:ecto, "~> 2.0"},
     {:postgrex, "~> 0.12.1"},
    ]
  end
. . .</code></pre>

<p>Fetch the dependencies and compile:</p>

<pre><code>$ mix do deps.get, compile</code></pre>

<p>And now we can add a repo for setup in <code>lib/repo.ex</code>:</p>

<pre><code>defmodule GenstageExample.Repo do
  use Ecto.Repo,
    otp_app: :genstage_example
end</code></pre>

<p>and with this we can set up our config next in <code>config/config.exs</code>:</p>

<pre><code>use Mix.Config

config :genstage_example, ecto_repos: [GenstageExample.Repo]

config :genstage_example, GenstageExample.Repo,
  adapter: Ecto.Adapters.Postgres,
  database: "genstage_example",
  username: "your_username",
  password: "your_password",
  hostname: "localhost",
  port: "5432"</code></pre>

<p>And if we add a supservisor to <code>lib/genstage_example.ex</code> we can now start working with the DB:</p>

<pre><code>. . .
  def start(_type, _args) do
    import Supervisor.Spec, warn: false

    children = [
      supervisor(GenstageExample.Repo, []),
      worker(GenstageExample.Producer, []),
    ]
  end
. . .</code></pre>

<p>But we should also make an interface to do that, so let's import our query interface and repo to the producer:</p>

<pre><code>. . .
  import Ecto.Query
  import GenstageExample.Repo
. . .</code></pre>

<p>Now we need to create our migration:</p>

<pre><code>$ mix ecto.gen.migration setup_tasks status:text payload:binary</code></pre>

<p>Now that we have a functional database, we can start storing things.<br>
Let's remove our change in Broadcaster, as we only were doing that to demonstrate that there are others outside the normal default in our Producer.</p>

<pre><code>. . .
  def init(counter) do
    {:producer, counter}
  end
. . .</code></pre>

<h3>Modelling the Rest of the Functionality</h3>

<p>Now that we have all this boilerplate work completed we should come up with a model to run all of this now that we have a simple wired-together producer/consumer model.<br>
At the end of the day we are trying to make a task runner.<br>
To do this, we probably want to abstract the interface for tasks and DB interfacing into their own modules.<br>
To start, let's create our <code>Task</code> module to model our actual tasks to be run:</p>

<pre><code>defmodule GenstageExample.Task do
  def enqueue(status, payload) do
    GenstageExample.TaskDBInterface.insert_tasks(status, payload)
  end

  def take(limit) do
    GenstageExample.TaskDBInterface.take_tasks(limit)
  end
end</code></pre>

<p>This is a <em>really</em> simple interface to abstract a given task's functionality.<br>
We only have 2 functions.<br>
Now, the module they are calling doesn't exist yet, it gives us the ideas we need to build a very simple interface.<br>
These can be broken down as follows:</p>

<ol>
<li><code>enqueue/2</code> - Enqueue a task to be run</li>
<li><code>take/1</code> - Take a given number of tasks to run from the database</li>
</ol>

<p>Now this gives us the interface we need: we can set things to be run, and grab tasks to be run and we can define the rest of the interface.<br>
Let's create an interface with our database in its own module:</p>

<pre><code>defmodule GenstageExample.TaskDBInterface do
  import Ecto.Query

  def take_tasks(limit) do
    {:ok, {count, events}} =
      GenstageExample.Repo.transaction fn ->
        ids = GenstageExample.Repo.all waiting(limit)
        GenstageExample.Repo.update_all by_ids(ids), [set: [status: "running"]], [returning: [:id, :payload]]
      end
    {count, events}
  end

  def insert_tasks(status, payload) do
    GenstageExample.Repo.insert_all "tasks", [
      %{status: status, payload: payload}
    ]
  end

  def update_task_status(id, status) do
    GenstageExample.Repo.update_all by_ids([id]), set: [status: status]
  end

  defp by_ids(ids) do
    from t in "tasks", where: t.id in ^ids
  end

  defp waiting(limit) do
    from t in "tasks",
      where: t.status == "waiting",
      limit: ^limit,
      select: t.id,
      lock: "FOR UPDATE SKIP LOCKED"
  end
end</code></pre>

<p>This one is a bit more complex, but we'll break it down piece by piece.<br>
We have 3 main functions, and 2 private helpers:</p>

<h4>Main Functions</h4>
<ol>
<li><code>take_tasks/1</code></li>
<li><code>insert_tasks/2</code></li>
<li><code>update_task_status/2</code></li>
</ol>

<p>With <code>take_tasks/1</code> we have the bulk of our logic.<br>
This function will be called to grab tasks we have queued to run them.<br>
Let's look at the code:</p>

<pre><code>. . .
  def take_tasks(limit) do
    {:ok, {count, events}} =
      GenstageExample.Repo.transaction fn ->
        ids = GenstageExample.Repo.all waiting(limit)
        GenstageExample.Repo.update_all by_ids(ids), [set: [status: "running"]], [returning: [:id, :payload]]
      end
    {count, events}
  end
. . .</code></pre>

<p>We do a few things here.<br>
First, we go in and we wrap everything in a transaction.<br>
This maintains state in the database so we avoid race conditions and other bad things.<br>
Inside here, we get the ids of all tasks waiting to be executed up to some limit, and set them to <code>running</code> as their status.<br>
We return the <code>count</code> of total tasks and the events to be run in the consumer.</p>

<p>Next we have <code>insert_tasks/2</code>:</p>

<pre><code>. . .
  def insert_tasks(status, payload) do
    GenstageExample.Repo.insert_all "tasks", [
      %{status: status, payload: payload}
    ]
  end
. . .</code></pre>

<p>This one is a bit more simple.<br>
We just insert a task to be run with a given payload binary.</p>

<p>Finally, we have <code>update_task_status/2</code>, which is also quite simple:</p>

<pre><code>. . .
  def update_task_status(id, status) do
    GenstageExample.Repo.update_all by_ids([id]), set: [status: status]
  end
. . .</code></pre>

<p>Here we simple update tasks to the status we want using a given id.</p>

<h4>Helpers</h4>
<p>Our helpers are all called primarily inside of <code>take_tasks/1</code>, but also used elsewhere in the main public API.</p>

<pre><code>. . .
  defp by_ids(ids) do
    from t in "tasks", where: t.id in ^ids
  end

  defp waiting(limit) do
    from t in "tasks",
      where: t.status == "waiting",
      limit: ^limit,
      select: t.id,
      lock: "FOR UPDATE SKIP LOCKED"
  end
. . .</code></pre>

<p>Neither of these has a ton of complexity.<br>
<code>by_ids/1</code> simply grabs all tasks that match in a given list of IDs.</p>

<p><code>waiting/1</code> finds all tasks that have the status waiting up to a given limit.<br>
However, there is one note to make on <code>waiting/1</code>.<br>
We leverage a lock on all tasks being updated so we skip those, a feature available in psql 9.5+.<br>
Outside of this, it is a very simple <code>SELECT</code> statement.</p>

<p>Now that we have our DB interface defined as it is used in the primary API, we can move onto the producer, consumer, and last bits of configuration.</p>

<h3>Producer, Consumer, and Final Configuration</h3>

<h4>Final Config</h4>
<p>We will need to do a bit of configuration in <code>lib/genstage_example.ex</code> to clarify things as well as give us the final functionalities we will need to run jobs.<br>
This is what we will end up with:</p>

<pre><code>. . .
  def start(_type, _args) do
    import Supervisor.Spec, warn: false
                          # 12 workers / system core
    consumers = for id <- (0..System.schedulers_online * 12) do
                  worker(GenstageExample.Consumer, [], id: id)
                end
    producers = [
                 worker(Producer, []),
                ]

    supervisors = [
                    supervisor(GenstageExample.Repo, []),
                    supervisor(Task.Supervisor, [[name: GenstageExample.TaskSupervisor]]),
                  ]
    children = supervisors ++ producers ++ consumers

    opts = [strategy: :one_for_one, name: GenstageExample.Supervisor]
    Supervisor.start_link(children, opts)
  end

  def start_later(module, function, args) do
    payload = {module, function, args} |> :erlang.term_to_binary
    Repo.insert_all("tasks", [
                              %{status: "waiting", payload: payload}
                             ])
    notify_producer
  end

  def notify_producer do
    send(Producer, :data_inserted)
  end

  defdelegate enqueue(module, function, args), to: Producer
. . .</code></pre>

<p>Let's tackle this from the top down.<br>
First, <code>start/2</code>:</p>

<pre><code>. . .
  def start(_type, _args) do
    import Supervisor.Spec, warn: false
                          # 12 workers / system core
    consumers = for id <- (0..System.schedulers_online * 12) do
                  worker(GenstageExample.Consumer, [], id: id)
                end
    producers = [
                 worker(Producer, []),
                ]

    supervisors = [
                    supervisor(GenstageExample.Repo, []),
                    supervisor(Task.Supervisor, [[name: GenstageExample.TaskSupervisor]]),
                  ]
    children = supervisors ++ producers ++ consumers

    opts = [strategy: :one_for_one, name: GenstageExample.Supervisor]
    Supervisor.start_link(children, opts)
  end
. . .</code></pre>

<p>First of all, you will notice we are now defining producers, consumers, and supervisors separately.<br>
I find this convention to work quite well to illustrate the intentions of various processes and trees we are starting here.<br>
In these 3 lists we set up 12 consumers / CPU core, set up a single producer, and then our supervisors for the Repo, as well as one new one.</p>

<p>This new supervisor is run through <code>Task.Supervisor</code>, which is built into Elixir.<br>
We give it a name so it is easily referred to in our GenStage code, <code>GenstageExample.TaskSupervisor</code>.<br>
Now, we define our children as the concatenation of all these lists.</p>

<p>Next, we have <code>start_later/3</code>:</p>

<pre><code>. . .
  def start_later(module, function, args) do
    payload = {module, function, args} |> :erlang.term_to_binary
    Repo.insert_all("tasks", [
                              %{status: "waiting", payload: payload}
                             ])
    notify_producer
  end
. . .</code></pre>

<p>This function takes a module, a function, and an argument.<br>
It then encodes them as a binary using some built-in erlang magic.<br>
From here, we then insert the task as <code>waiting</code>, and we notify a producer that a task has been inserted to run.</p>

<p>Now let's check out <code>notify_producer/0</code>:</p>

<pre><code>. . .
  def notify_producer do
    send(Producer, :data_inserted)
  end
. . .</code></pre>

<p>This method is quite simple.<br>
We send our producer a message, <code>:data_inserted</code>, simply so that it knows what we did.<br>
The message here is arbitrary, but I chose this atom to make the meaning clear.</p>

<p>Last, but not least we do some simple delegation:</p>

<pre><code>. . .
  defdelegate enqueue(module, functions, args), to : Producer
. . .</code></pre>

<p>This simply makes it so if we call <code>GenstageExample.enqueue(module, function, args)</code> that it will be delegated to the same method in our producer.</p>

<h3>Producer Setup</h3>
<p>Our producer doesn't need a ton of work.<br>
first, we'll alter our <code>handle_demand/2</code> to actually do something with our events:</p>

<pre><code>. . .
  def handle_demand(demand, state) when demand > 0 do
    serve_jobs(demand + state)
  end
. . .</code></pre>

<p>We haven't defined <code>serve_jobs/2</code> yet, but we'll get there.<br>
The concept is simple, when we get a demand and demand is > 0, we do some work to the tune of demand + the current state's number of jobs.</p>

<p>Now that we will be sending a message to the producer when we run <code>start_later/3</code>, we will want to respond to it with a <code>handle_info/2</code> call:</p>

<pre><code>. . .
  def handle_info(:enqueued, state) do
    {count, events} = GenstageExample.Task.take(state)
    {:noreply, events, state - count}
  end
. . .</code></pre>

<p>With this, we simply respond by taking the number of tasks we are told to get ready to run.</p>

<p>Now let's define <code>serve_jobs/1</code>:</p>

<pre><code>. . .
  def serve_jobs limit do
    {count, events} = GenstageExample.Task.take(limit)
    Process.send_after(@name, :enqueued, 60_000)
    {:noreply, events, limit - count}
  end
. . .</code></pre>

<p>Now, we are sending a process in one minute that to our producer telling it that it should respond to <code>:enqueued</code>.<br>
Note that we call the process module with <code>@name</code>, which we will need to add at the top as a module attribute:</p>

<pre><code>. . .
  @name __MODULE__
. . .</code></pre>

<p>Let's define that last function to handle the <code>:enqueued</code> message now, too:</p>

<pre><code>. . .
  def handle_cast(:enqueued, state) do
    serve_jobs(state)
  end
. . .</code></pre>

<p>This will simply serve jobs when we tell the producer they have <code>state</code> number of enqueued and to respond.</p>

<h2>Setting Up the Consumer for Real Work</h2>
<p>Our consumer is where we do the work.<br>
Now that we have our producer storing tasks, we want to have the consumer handle this as well.<br>
There is a good bit of work to be done here tying into our work so far.<br>
The core of the consumer is <code>handle_events/3</code>, lets flesh out the functionality we wish to have there and define it as we go further:</p>

<pre><code>. . .
  def handle_events(events, _from, state) do
    for event <- events do
      %{id: id, payload: payload} = event
      {module, function, args} = payload |> deconstruct_payload
      task = start_task(module, function, args)
      yield_to_and_update_task(task, id)
    end
    {:noreply, [], state}
  end
. . .</code></pre>

<p>At its core, this setup simple just wants to run a task we decode the binary of.<br>
To do this we get the data from the event, deconstruct it, and then start and yield to a task.<br>
These functions aren't defined yet, so let's create them:</p>

<pre><code>. . .
  def deconstruct_payload payload do
    payload |> :erlang.binary_to_term
  end
. . .</code></pre>

<p>We can use Erlang's built-in inverse of our other <code>term_to_binary/1</code> function to get our module, function, and args back out.<br>
Now we need to start the task:</p>

<pre><code>. . .
  def start_task(mod, func, args) do
    Task.Supervisor.async_nolink(TaskSupervisor, mod, func, args)
  end
. . .</code></pre>

<p>Here we leverage the supervisor we created at the beginning to run this in a task.<br>
Now we need to define <code>yield_to_and_update_task/2</code>:</p>

<pre><code>. . .
  def yield_to_and_update_task(task, id) do
    task
    |> Task.yield(1000)
    |> yield_to_status(task)
    |> update(id)
  end
. . .</code></pre>

<p>Now this brings in more pieces we've yet to define, but the core is simple.<br>
We wait 1 second for the task to run.<br>
From here, we respond to the status it returns (which will either be <code>:ok</code>, <code>:exit</code>, or <code>nil</code>) and handle it as such.<br>
After that we update our task via our DB interface to get things current.<br>
Let's define <code>yield_to_status/2</code> for each of the scenarios we mentioned:</p>

<pre><code>. . .
  def yield_to_status({:ok, _}, _) do
    "success"
  end

  def yield_to_status({:exit, _}, _) do
    "error"
  end

  def yield_to_status(nil, task) do
    Task.shutdown(task)
    "timeout"
  end
. . .</code></pre>

<p>These simple handle the atom being returned from the process and respond appropriately.<br>
If it takes more than a second, we need to shut it down because otherwise it would just hang forever.</p>

<p>If we make another method to update the database after consumption, we are set to go:</p>

<pre><code>. . .
  defp update(status, id) do
    GenstageExample.TaskDBInterface.update_task_status(id, status)
  end
. . .</code></pre>

<p>And here we just call it through our database interface and update the status after yielding to allow the task time to run.</p>

<p>From this, we can see our finalized consumer:</p>

<pre><code>defmodule GenstageExample.Consumer do
  alias Experimental.GenStage
  use GenStage
  alias GenstageExample.{Producer, TaskSupervisor}

  def start_link do
    GenStage.start_link(__MODULE__, :state_doesnt_matter)
  end

  def init(state) do
    {:consumer, state, subscribe_to: [Producer]}
  end

  def handle_events(events, _from, state) do
    for event <- events do
      %{id: id, payload: payload} = event
      {module, function, args} = payload |> deconstruct_payload
      task = start_task(module, function, args)
      yield_to_and_update_task(task, id)
    end
    {:noreply, [], state}
  end

  defp yield_to_and_update_task(task, id) do
    task
    |> Task.yield(1000)
    |> yield_to_status(task)
    |> update(id)
  end

  defp start_task(mod, func, args) do
    Task.Supervisor.async_nolink(TaskSupervisor, mod  , func, args)
  end

  defp yield_to_status({:ok, _}, _) do
    "success"
  end

  defp yield_to_status({:exit, _}, _) do
    "error"
  end

  defp yield_to_status(nil, task) do
    Task.shutdown(task)
    "timeout"
  end

  defp update(status, id) do
    GenstageExample.TaskDBInterface.update_task_status(id, status)
  end

  defp deconstruct_payload payload do
    payload |> :erlang.binary_to_term
  end
end</code></pre>

<p>Now, if we go into IEx:</p>

<pre><code>$ iex -S mix
iex> GenstageExample.enqueue(IO, :puts, ["wuddup"])
#=>
16:39:31.014 [debug] QUERY OK db=137.4ms
INSERT INTO "tasks" ("payload","status") VALUES ($1,$2) [<<131, 104, 3, 100, 0, 9, 69, 108, 105, 120, 105, 114, 46, 73, 79, 100, 0, 4, 112, 117, 116, 115, 108, 0, 0, 0, 1, 109, 0, 0, 0, 6, 119, 117, 100, 100, 117, 112, 106>>, "waiting"]
:ok

16:39:31.015 [debug] QUERY OK db=0.4ms queue=0.1ms
begin []

16:39:31.025 [debug] QUERY OK source="tasks" db=9.6ms
SELECT t0."id" FROM "tasks" AS t0 WHERE (t0."status" = 'waiting') LIMIT $1 FOR UPDATE SKIP LOCKED [49000]

16:39:31.026 [debug] QUERY OK source="tasks" db=0.8ms
UPDATE "tasks" AS t0 SET "status" = $1 WHERE (t0."id" = ANY($2)) RETURNING t0."id", t0."payload" ["running", [5]]

16:39:31.040 [debug] QUERY OK db=13.5ms
commit []
iex(2)> wuddup

16:39:31.060 [debug] QUERY OK source="tasks" db=1.3ms
UPDATE "tasks" AS t0 SET "status" = $1 WHERE (t0."id" = ANY($2)) ["success", [5]]</code></pre>

<p>It works and we are storing and running tasks!</p>
`;

        const robotVersion = `
<h1>Writing A Job Runner (In Elixir)</h1>

<p>We're building a job runner to learn about distributed systems. Not just any job runner - one that can serialize and execute arbitrary code across systems. Think Faktory, but we'll understand every line.</p>

<p>Elixir makes this uniquely simple. In most languages, you'd serialize job data as JSON, then reconstruct objects and call methods. Elixir can serialize actual function calls as binaries and execute them elsewhere. The Erlang Term Format isn't just data - it's executable code.</p>

<p>Building this teaches fundamental distributed systems concepts: work distribution, failure handling, exactly-once processing. But it's also just fun. There's something satisfying about code that runs other code, systems that heal themselves, and workers that can't corrupt each other even if they try.</p>

<p>Let's build something real and learn by doing.</p>

<div class="aside collapsed">
<div class="aside-header" onclick="toggleAside(this.parentElement)">
    <span class="aside-label">Aside 1</span>
    <span class="aside-title">The Landscape of Job Processing</span>
</div>
<div class="aside-content">
<h2>The Landscape of Job Processing</h2>

<p>In Ruby, you might reach for Sidekiq. It's battle-tested, using Redis for storage and threads for concurrency. Jobs are JSON objects, workers pull from queues, and if something crashes, you hope your monitoring catches it. It works well until you need to scale beyond a single Redis instance or handle complex job dependencies.</p>

<p>Python developers often turn to Celery. It's more distributed by design, supporting multiple brokers and result backends. But the complexity shows - you're configuring RabbitMQ, dealing with serialization formats, and debugging issues across multiple moving parts. When a worker dies mid-job, recovery depends on how well you've configured acknowledgments and retries.</p>

<p>Go developers might use machinery or asynq, leveraging goroutines for concurrency. The static typing helps catch errors early, but you're still manually managing worker pools and carefully handling panics to prevent the whole process from dying.</p>

<p>Each solution reflects its language's strengths and limitations. They all converge on similar patterns: a persistent queue, worker processes, and lots of defensive programming. What if the language itself provided better primitives for this problem?</p>
</div>
</div>

<div class="aside collapsed">
<div class="aside-header" onclick="toggleAside(this.parentElement)">
    <span class="aside-label">Aside 2</span>
    <span class="aside-title">Why Elixir Works for Job Processing</span>
</div>
<div class="aside-content">
<h2>Why Elixir Works for Job Processing</h2>

<p><strong>Processes are the unit of concurrency.</strong> Not threads, not coroutines - processes. Each process has its own heap, runs concurrently, and can't corrupt another's memory. Starting one is measured in microseconds and takes about 2KB of memory. You don't manage a pool of workers; you spawn a process per job.</p>

<p><strong>Failure is isolated by default.</strong> When a process crashes, it dies alone. No corrupted global state, no locked mutexes, no zombie threads. The supervisor sees the death, logs it, and starts a fresh process. Your job processor doesn't need defensive try-catch blocks everywhere - it needs a good supervision tree.</p>

<p><strong>Message passing is the only way to communicate.</strong> No shared memory means no locks, no race conditions, no memory barriers. A process either receives a message or it doesn't. This constraint simplifies concurrent programming dramatically - you can reason about each process in isolation.</p>

<p><strong>The scheduler handles fairness.</strong> The BEAM VM runs its own scheduler, preemptively switching between processes every 2000 reductions. One process can't starve others by hogging the CPU. This is why Phoenix can handle millions of WebSocket connections - each connection is just another lightweight process.</p>

<p><strong>Distribution is built-in.</strong> Connect nodes with one function call. Send messages across the network with the same syntax as local messages. The Erlang Term Format serializes any data structure, including function references. Your job queue can span multiple machines without changing the core logic.</p>

<p><strong>Hot code reloading works.</strong> Deploy new code without stopping the system. The BEAM can run two versions of a module simultaneously, migrating processes gracefully. Your job processor can be upgraded while it's processing jobs.</p>

<p><strong>Introspection is exceptional.</strong> Connect to a running system and inspect any process. See its message queue, memory usage, current function. The observer GUI shows your entire system's health in real-time. When production misbehaves, you can debug it live.</p>

<p>These aren't features bolted on top - they're fundamental to how the BEAM VM works. When you build a job processor in Elixir, you're not fighting the language to achieve reliability and concurrency. You're using it as designed.</p>
</div>
</div>

<h1>Thinking About Job Runners, Producers, Consumers, and Events</h1>

<h2>The Architecture of Work</h2>

<p>A job runner is fundamentally about choreographing work. You have tasks that need to be done, workers that can do them, and a system that connects the two. But the devil is in the details: how do you ensure work gets distributed fairly? How do you handle failures? How do you prevent workers from being overwhelmed?</p>

<p>GenStage answers these questions with a demand-driven architecture. Instead of pushing work to workers (and hoping they can handle it), workers pull work when they're ready. This simple inversion changes everything.</p>

<h2>Understanding Producer-Consumer Patterns</h2>

<p>The producer-consumer pattern isn't unique to Elixir. It's a fundamental pattern in distributed systems:</p>

<p><strong>In Apache Spark</strong>, RDDs (Resilient Distributed Datasets) flow through transformations. Each transformation is essentially a consumer of the previous stage and a producer for the next. Spark handles backpressure through its task scheduler - if executors are busy, new tasks wait.</p>

<p><strong>In Kafka Streams</strong>, topics act as buffers between producers and consumers. Consumers track their offset, pulling messages at their own pace. The broker handles persistence and replication.</p>

<p><strong>In Go channels</strong>, goroutines communicate through typed channels. A goroutine blocks when sending to a full channel or receiving from an empty one. This provides natural backpressure but requires careful capacity planning.</p>

<p>GenStage takes a different approach. There are no intermediate buffers or brokers. Producers and consumers negotiate directly:</p>

<ol>
<li>Consumer asks producer for work (specifying how much it can handle)</li>
<li>Producer responds with up to that many events</li>
<li>Consumer processes events and asks for more</li>
</ol>

<p>This creates a pull-based system with automatic flow control. No queues filling up, no brokers to manage, no capacity planning. The system self-regulates based on actual processing speed.</p>

<h2>What We're Actually Building</h2>

<p>Our job runner will have three core components:</p>

<p><strong>Producers</strong> - These generate or fetch work. In our case, they'll pull jobs from a database table. A producer doesn't decide who gets work - it simply responds to demand. When a consumer asks for 10 jobs, the producer queries the database for 10 unclaimed jobs and returns them.</p>

<p><strong>Consumers</strong> - These execute jobs. Each consumer is a separate Elixir process, isolated from others. When a consumer is ready for work, it asks its producer for events. After processing, it asks for more. If a consumer crashes while processing a job, only that job is affected.</p>

<p><strong>Events</strong> - The unit of work flowing through the system. In GenStage, everything is an event. For our job runner, an event is a job to be executed. Events flow from producers to consumers based on demand, never faster than consumers can handle.</p>

<h2>The Beauty of Modeling Everything as Events</h2>

<p>When you model work as events, powerful patterns emerge:</p>

<p><strong>Composition</strong> - You can chain stages together. A consumer can also be a producer for another stage. Want to add a step that enriches jobs before execution? Insert a producer-consumer between your current stages.</p>

<p><strong>Fan-out/Fan-in</strong> - One producer can feed multiple consumers (fan-out). Multiple producers can feed one consumer (fan-in). The demand mechanism ensures fair distribution.</p>

<p><strong>Buffering</strong> - Need a buffer? Add a producer-consumer that accumulates events before passing them on. The buffer only fills as fast as downstream consumers can drain it.</p>

<p><strong>Filtering</strong> - A producer-consumer can selectively forward events. Only want to process high-priority jobs? Filter them in a middle stage.</p>

<div style="margin: 30px 0; text-align: center;">
<svg width="800" height="300" viewBox="0 0 800 300" style="border: 1px solid #eee; background: #fafafa;">
  <!-- Social Media Sources -->
  <!-- BlueSky -->
  <ellipse cx="60" cy="80" rx="40" ry="20" fill="#0085ff" stroke="#000" stroke-width="1"/>
  <text x="60" y="85" text-anchor="middle" fill="white" font-size="12" font-weight="bold">BlueSky</text>

  <!-- Twitter -->
  <ellipse cx="60" cy="150" rx="40" ry="20" fill="#1da1f2" stroke="#000" stroke-width="1"/>
  <text x="60" y="155" text-anchor="middle" fill="white" font-size="12" font-weight="bold">Twitter</text>

  <!-- TikTok -->
  <ellipse cx="60" cy="220" rx="40" ry="20" fill="#ff0050" stroke="#000" stroke-width="1"/>
  <text x="60" y="225" text-anchor="middle" fill="white" font-size="12" font-weight="bold">TikTok</text>

  <!-- Arrows from sources to Producer -->
  <defs>
    <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="10" refY="3.5" orient="auto">
      <polygon points="0 0, 10 3.5, 0 7" fill="#333"/>
    </marker>
  </defs>

  <!-- BlueSky to Producer -->
  <line x1="100" y1="80" x2="170" y2="120" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>
  <text x="135" y="95" font-size="10" fill="#666">post</text>

  <!-- Twitter to Producer -->
  <line x1="100" y1="150" x2="170" y2="150" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>
  <text x="135" y="145" font-size="10" fill="#666">post</text>

  <!-- TikTok to Producer -->
  <line x1="100" y1="220" x2="170" y2="180" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>
  <text x="135" y="205" font-size="10" fill="#666">post</text>

  <!-- Producer Box -->
  <rect x="180" y="120" width="100" height="60" fill="#4CAF50" stroke="#000" stroke-width="2" rx="5"/>
  <text x="230" y="155" text-anchor="middle" fill="white" font-size="14" font-weight="bold">Producer</text>

  <!-- Multiple arrows from Producer to ProducerConsumer -->
  <line x1="280" y1="140" x2="350" y2="140" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>
  <line x1="280" y1="150" x2="350" y2="150" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>
  <line x1="280" y1="160" x2="350" y2="160" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>

  <!-- ProducerConsumer Box -->
  <rect x="360" y="120" width="120" height="60" fill="#ff9800" stroke="#000" stroke-width="2" rx="5"/>
  <text x="420" y="145" text-anchor="middle" fill="white" font-size="12" font-weight="bold">ProducerConsumer</text>
  <text x="420" y="160" text-anchor="middle" fill="white" font-size="10">apply transformation</text>
  <text x="420" y="172" text-anchor="middle" fill="white" font-size="10">if needed</text>

  <!-- Arrows from ProducerConsumer to Consumer -->
  <line x1="480" y1="140" x2="550" y2="140" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>
  <line x1="480" y1="150" x2="550" y2="150" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>
  <line x1="480" y1="160" x2="550" y2="160" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>

  <!-- Consumer Box -->
  <rect x="560" y="120" width="80" height="60" fill="#2196F3" stroke="#000" stroke-width="2" rx="5"/>
  <text x="600" y="155" text-anchor="middle" fill="white" font-size="14" font-weight="bold">Consumer</text>

  <!-- Arrow to Database -->
  <line x1="640" y1="150" x2="690" y2="150" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>

  <!-- Database Cylinder -->
  <ellipse cx="730" cy="140" rx="30" ry="15" fill="#9C27B0" stroke="#000" stroke-width="1"/>
  <rect x="700" y="140" width="60" height="30" fill="#9C27B0" stroke="#000" stroke-width="1"/>
  <ellipse cx="730" cy="170" rx="30" ry="15" fill="#9C27B0" stroke="#000" stroke-width="1"/>
  <text x="730" y="160" text-anchor="middle" fill="white" font-size="11" font-weight="bold">Database</text>

  <!-- Labels -->
  <text x="400" y="30" text-anchor="middle" font-size="16" font-weight="bold" fill="#333">Event Flow Pipeline</text>
  <text x="400" y="280" text-anchor="middle" font-size="12" fill="#666">Social media posts â†’ Producer â†’ Transformation â†’ Consumer â†’ Storage</text>
</svg>
</div>

<div style="margin: 30px 0; text-align: center;">
<svg width="800" height="300" viewBox="0 0 800 300" style="border: 1px solid #eee; background: #fafafa;">
  <!-- Social Media Sources -->
  <!-- BlueSky -->
  <ellipse cx="60" cy="80" rx="40" ry="20" fill="#0085ff" stroke="#000" stroke-width="1"/>
  <text x="60" y="85" text-anchor="middle" fill="white" font-size="12" font-weight="bold">BlueSky</text>

  <!-- Twitter -->
  <ellipse cx="60" cy="150" rx="40" ry="20" fill="#1da1f2" stroke="#000" stroke-width="1"/>
  <text x="60" y="155" text-anchor="middle" fill="white" font-size="12" font-weight="bold">Twitter</text>

  <!-- TikTok -->
  <ellipse cx="60" cy="220" rx="40" ry="20" fill="#ff0050" stroke="#000" stroke-width="1"/>
  <text x="60" y="225" text-anchor="middle" fill="white" font-size="12" font-weight="bold">TikTok</text>

  <!-- Arrows from sources to Producer -->
  <defs>
    <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="10" refY="3.5" orient="auto">
      <polygon points="0 0, 10 3.5, 0 7" fill="#333"/>
    </marker>
  </defs>

  <!-- BlueSky to Producer -->
  <line x1="100" y1="80" x2="170" y2="120" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>
  <text x="135" y="95" font-size="10" fill="#666">post</text>

  <!-- Twitter to Producer -->
  <line x1="100" y1="150" x2="170" y2="150" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>
  <text x="135" y="145" font-size="10" fill="#666">post</text>

  <!-- TikTok to Producer -->
  <line x1="100" y1="220" x2="170" y2="180" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>
  <text x="135" y="205" font-size="10" fill="#666">post</text>

  <!-- Producer Box -->
  <rect x="180" y="120" width="100" height="60" fill="#4CAF50" stroke="#000" stroke-width="2" rx="5"/>
  <text x="230" y="155" text-anchor="middle" fill="white" font-size="14" font-weight="bold">Producer</text>

  <!-- Multiple arrows from Producer to ProducerConsumer -->
  <line x1="280" y1="140" x2="350" y2="140" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>
  <line x1="280" y1="150" x2="350" y2="150" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>
  <line x1="280" y1="160" x2="350" y2="160" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>

  <!-- ProducerConsumer Box -->
  <rect x="360" y="120" width="120" height="60" fill="#ff9800" stroke="#000" stroke-width="2" rx="5"/>
  <text x="420" y="145" text-anchor="middle" fill="white" font-size="12" font-weight="bold">ProducerConsumer</text>
  <text x="420" y="160" text-anchor="middle" fill="white" font-size="10">apply transformation</text>
  <text x="420" y="172" text-anchor="middle" fill="white" font-size="10">if needed</text>

  <!-- Arrows from ProducerConsumer to Consumer -->
  <line x1="480" y1="140" x2="550" y2="140" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>
  <line x1="480" y1="150" x2="550" y2="150" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>
  <line x1="480" y1="160" x2="550" y2="160" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>

  <!-- Consumer Box -->
  <rect x="560" y="120" width="80" height="60" fill="#2196F3" stroke="#000" stroke-width="2" rx="5"/>
  <text x="600" y="155" text-anchor="middle" fill="white" font-size="14" font-weight="bold">Consumer</text>

  <!-- Arrow to Database -->
  <line x1="640" y1="150" x2="690" y2="150" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>

  <!-- Database Cylinder -->
  <ellipse cx="730" cy="140" rx="30" ry="15" fill="#9C27B0" stroke="#000" stroke-width="1"/>
  <rect x="700" y="140" width="60" height="30" fill="#9C27B0" stroke="#000" stroke-width="1"/>
  <ellipse cx="730" cy="170" rx="30" ry="15" fill="#9C27B0" stroke="#000" stroke-width="1"/>
  <text x="730" y="160" text-anchor="middle" fill="white" font-size="11" font-weight="bold">Database</text>

  <!-- Labels -->
  <text x="400" y="30" text-anchor="middle" font-size="16" font-weight="bold" fill="#333">Event Flow Pipeline</text>
  <text x="400" y="280" text-anchor="middle" font-size="12" fill="#666">Social media posts â†’ Producer â†’ Transformation â†’ Consumer â†’ Storage</text>
</svg>
</div>

<h2>Why This Matters for Job Processing</h2>

<p>Traditional job processors push jobs into queues. Workers poll these queues, hoping to grab work. This creates several problems:</p>

<ol>
<li><strong>Queue overflow</strong> - Producers can overwhelm the queue if consumers are slow</li>
<li><strong>Unfair distribution</strong> - Fast workers might grab all the work</li>
<li><strong>Visibility</strong> - Hard to see where bottlenecks are</li>
<li><strong>Error handling</strong> - What happens to in-flight jobs when a worker dies?</li>
</ol>

<p>GenStage's demand-driven model solves these elegantly:</p>

<ol>
<li><strong>No overflow</strong> - Producers only generate what's demanded</li>
<li><strong>Fair distribution</strong> - Each consumer gets what it asks for</li>
<li><strong>Clear bottlenecks</strong> - Slow stages naturally build up demand</li>
<li><strong>Clean errors</strong> - Crashed consumers simply stop demanding; their work remains unclaimed</li>
</ol>

<p>This isn't theoretical. Telecom systems have used these patterns for decades. When you make a phone call, switches don't push calls through the network - each hop pulls when ready. This prevents network overload even during disasters when everyone tries to call at once.</p>

<p>We're applying the same battle-tested patterns to job processing. The result is a system that's naturally resilient, self-balancing, and surprisingly simple to reason about.</p>

<p>Ready to see how this translates to code? Let's build our first producer.</p>

<h1>Building the Foundation</h1>

<h2>Step 1: Creating Your Phoenix Project</h2>

<p>Let's start fresh with a new Phoenix project. Open your terminal and run:</p>

<pre><code>mix phx.new job_processor --no-dashboard --no-mailer
cd job_processor</code></pre>

<p>We're keeping it lean - no dashboard or mailer for now. When prompted to install dependencies, say yes.</p>

<p>Why Phoenix? We're not building a web app, but Phoenix gives us:</p>
<ul>
<li>A supervision tree already set up</li>
<li>Configuration management</li>
<li>A database connection (Ecto)</li>
<li>LiveView for our monitoring dashboard (later)</li>
</ul>

<p>Think of Phoenix as our application framework, not just a web framework.</p>

<h2>Step 2: Adding GenStage</h2>

<p>Open <code>mix.exs</code> and add GenStage to your dependencies:</p>

<pre><code>defp deps do
  [
    {:phoenix, "~> 1.7.12"},
    {:phoenix_ecto, "~> 4.5"},
    {:ecto_sql, "~> 3.11"},
    {:postgrex, ">= 0.0.0"},
    {:phoenix_html, "~> 4.1"},
    {:phoenix_live_reload, "~> 1.2", only: :dev},
    {:phoenix_live_view, "~> 0.20.14"},
    {:telemetry_metrics, "~> 0.6"},
    {:telemetry_poller, "~> 1.0"},
    {:jason, "~> 1.2"},
    {:bandit, "~> 1.2"},

    # Add this line
    {:gen_stage, "~> 1.2"}
  ]
end</code></pre>

<p>Now fetch the dependency:</p>

<pre><code>mix deps.get</code></pre>

<p>That's it. One dependency. GenStage is maintained by the Elixir core team, so it follows the same design principles as the language itself.</p>

<h2>Step 3: Understanding GenStage's Mental Model</h2>

<p>Before we write code, let's cement the mental model. GenStage orchestrates three types of processes:</p>

<p><strong>Producers</strong> emit events. They don't push events anywhere - they hold them until a consumer asks. Think of a producer as a lazy river of data. The water (events) only flows when someone downstream opens a valve (demands).</p>

<p><strong>Consumers</strong> receive events. They explicitly ask producers for a specific number of events. This is the key insight: consumers control the flow rate, not producers.</p>

<p><strong>Producer-Consumers</strong> do both. They receive events from upstream, transform them, and emit to downstream. Perfect for building pipelines.</p>

<p>Every GenStage process follows this lifecycle:</p>
<ol>
<li>Start and connect to other stages</li>
<li>Consumer sends demand upstream</li>
<li>Producer receives demand and emits events</li>
<li>Consumer receives and processes events</li>
<li>Repeat from step 2</li>
</ol>

<p>The demand mechanism is what makes this special. In a traditional queue, you might have:</p>

<pre><code># Traditional approach - producer decides when to push
loop do
  job = create_job()
  Queue.push(job)  # What if queue is full?
end</code></pre>

<p>With GenStage:</p>

<pre><code># GenStage approach - consumer decides when to pull
def handle_demand(demand, state) do
  jobs = create_jobs(demand)  # Only create what's asked for
  {:noreply, jobs, state}
end</code></pre>

<p>The consumer is in control. It's impossible to overwhelm a consumer because it only gets what it asked for.</p>

<h2>Step 4: Creating the Producer</h2>

<p>Now for the meat of it. Let's build a producer that understands our job processing needs. Create a new file at <code>lib/job_processor/producer.ex</code>:</p>

<pre><code>defmodule JobProcessor.Producer do
  use GenStage
  require Logger

  @doc """
  Starts the producer with an initial state.

  The state can be anything, but we'll use a counter to start simple.
  """
  def start_link(initial \\ 0) do
    GenStage.start_link(__MODULE__, initial, name: __MODULE__)
  end

  @impl true
  def init(counter) do
    Logger.info("Producer starting with counter: #{counter}")
    {:producer, counter}
  end

  @impl true
  def handle_demand(demand, state) do
    Logger.info("Producer received demand for #{demand} events")

    # Generate events to fulfill demand
    events = Enum.to_list(state..(state + demand - 1))

    # Update our state
    new_state = state + demand

    # Return events and new state
    {:noreply, events, new_state}
  end
end</code></pre>

<p>Let's dissect this line by line:</p>

<p><strong><code>use GenStage</code></strong> - This macro brings in the GenStage behavior. It's like <code>use GenServer</code> but for stages. It requires us to implement certain callbacks.</p>

<p><strong><code>start_link/1</code></strong> - Standard OTP pattern. We name the process after its module so we can find it easily. In production, you might want multiple producers, so you'd make the name configurable.</p>

<p><strong><code>init/1</code></strong> - The crucial part: <code>{:producer, counter}</code>. The first element declares this as a producer. The second is our initial state. GenStage now knows this process will emit events when asked.</p>

<p><strong><code>handle_demand/2</code></strong> - The heart of a producer. This callback fires when consumers ask for events. The arguments are:</p>
<ul>
<li><code>demand</code> - How many events the consumer wants</li>
<li><code>state</code> - Our current state</li>
</ul>

<p>The return value <code>{:noreply, events, new_state}</code> means:</p>
<ul>
<li><code>:noreply</code> - We're responding to demand, not a synchronous call</li>
<li><code>events</code> - The list of events to emit (must be a list)</li>
<li><code>new_state</code> - Our updated state</li>
</ul>

<h3>The Demand Buffer</h3>

<p>Here's something subtle but important: GenStage maintains an internal demand buffer. If multiple consumers ask for events before you can fulfill them, GenStage aggregates the demand.</p>

<p>For example:</p>
<ol>
<li>Consumer A asks for 10 events</li>
<li>Consumer B asks for 5 events</li>
<li>Your <code>handle_demand/2</code> receives demand for 15 events</li>
</ol>

<p>This batching is efficient and prevents your producer from being called repeatedly for small demands.</p>

<h3>What if You Can't Fulfill Demand?</h3>

<p>Sometimes you can't produce as many events as demanded. That's fine:</p>

<pre><code>def handle_demand(demand, state) do
  available = calculate_available_work()

  if available >= demand do
    events = fetch_events(demand)
    {:noreply, events, state}
  else
    # Can only partially fulfill demand
    events = fetch_events(available)
    {:noreply, events, state}
  end
end</code></pre>

<p>GenStage tracks unfulfilled demand. If you return fewer events than demanded, it remembers. The next time you have events available, you can emit them even without new demand:</p>

<pre><code>def handle_info(:new_data_available, state) do
  events = fetch_available_events()
  {:noreply, events, state}
end</code></pre>

<h3>Producer Patterns</h3>

<p>Our simple counter producer is just the beginning. Real-world producers follow several patterns:</p>

<p><strong>Database Polling Producer:</strong></p>
<pre><code>def handle_demand(demand, state) do
  jobs = Repo.all(
    from j in Job,
    where: j.status == "pending",
    limit: ^demand,
    lock: "FOR UPDATE SKIP LOCKED"
  )

  job_ids = Enum.map(jobs, & &1.id)

  Repo.update_all(
    from(j in Job, where: j.id in ^job_ids),
    set: [status: "processing"]
  )

  {:noreply, jobs, state}
end</code></pre>

<p><strong>Rate-Limited Producer:</strong></p>
<pre><code>def handle_demand(demand, %{rate_limit: limit} = state) do
  now = System.monotonic_time(:millisecond)
  time_passed = now - state.last_emit

  allowed = min(demand, div(time_passed * limit, 1000))

  if allowed > 0 do
    events = generate_events(allowed)
    {:noreply, events, %{state | last_emit: now}}
  else
    # Schedule retry
    Process.send_after(self(), :retry_demand, 100)
    {:noreply, [], state}
  end
end</code></pre>

<p><strong>Buffering Producer:</strong></p>
<pre><code>def handle_demand(demand, %{buffer: buffer} = state) do
  {to_emit, remaining} = Enum.split(buffer, demand)

  if length(to_emit) < demand do
    # Buffer exhausted, try to refill
    new_events = fetch_more_events()
    all_events = to_emit ++ new_events
    {to_emit_now, to_buffer} = Enum.split(all_events, demand)
    {:noreply, to_emit_now, %{state | buffer: to_buffer}}
  else
    {:noreply, to_emit, %{state | buffer: remaining}}
  end
end</code></pre>

<h3>Testing Your Producer</h3>

<p>Let's make sure our producer works. Create <code>test/job_processor/producer_test.exs</code>:</p>

<pre><code>defmodule JobProcessor.ProducerTest do
  use ExUnit.Case
  alias JobProcessor.Producer

  test "producer emits events on demand" do
    {:ok, producer} = Producer.start_link(0)

    # Manually subscribe and ask for events
    {:ok, _subscription} = GenStage.sync_subscribe(self(), to: producer, max_demand: 5)

    # We should receive 5 events (0 through 4)
    assert_receive {:"$gen_consumer", {_, _}, [0, 1, 2, 3, 4]}
  end

  test "producer maintains state across demands" do
    {:ok, producer} = Producer.start_link(10)

    # First demand
    {:ok, _} = GenStage.sync_subscribe(self(), to: producer, max_demand: 3)
    assert_receive {:"$gen_consumer", {_, _}, [10, 11, 12]}

    # Second demand should continue from where we left off
    send(producer, {:"$gen_producer", {self(), nil}, {:ask, 2}})
    assert_receive {:"$gen_consumer", {_, _}, [13, 14]}
  end
end</code></pre>

<p>Run the tests with <code>mix test</code>.</p>

<h3>The Power of Stateful Producers</h3>

<p>Our producer maintains state - a simple counter. But state can be anything:</p>

<ul>
<li>A database connection for polling</li>
<li>A buffer of pre-fetched events</li>
<li>Rate limiting information</li>
<li>Metrics and telemetry data</li>
</ul>

<p>Because each producer is just an Erlang process, it's isolated. If one producer crashes, others continue. The supervisor restarts the crashed producer with a fresh state.</p>

<p>This is different from thread-based systems where shared state requires locks. Each producer owns its state exclusively. No locks, no race conditions, no defensive programming.</p>

<h3>What We've Built</h3>

<p>Our producer is deceptively simple, but it demonstrates core principles:</p>

<ol>
<li><strong>Demand-driven</strong> - Only produces when asked</li>
<li><strong>Stateful</strong> - Maintains its own isolated state</li>
<li><strong>Supervised</strong> - Can crash and restart safely</li>
<li><strong>Testable</strong> - Easy to verify behavior</li>
</ol>

<p>In the next section, we'll build consumers that process these events. But the producer is the foundation - it controls the flow of work through our system.</p>

<h1>Building A Consumer</h1>

<p>Now that we have a producer emitting events, we need something to consume them. This is where consumers come in - they're the workers that actually process the events flowing through our system.</p>

<p>But here's the beautiful thing about GenStage consumers: they're not passive recipients waiting for work to be thrown at them. They're active participants in the flow control. A consumer decides how much work it can handle and explicitly asks for that amount. No more, no less.</p>

<p>Think about how this changes the dynamics. In a traditional message queue, producers blast messages into a queue, hoping consumers can keep up. If consumers fall behind, the queue grows. If consumers are faster than expected, they sit idle waiting for work. It's a constant balancing act with lots of manual tuning.</p>

<p>GenStage flips this completely. Consumers know their own capacity better than anyone else. They know if they're currently processing a heavy job, if they're running low on memory, or if they're about to restart. So they ask for exactly what they can handle right now.</p>

<h2>The Consumer's Lifecycle</h2>

<p>A GenStage consumer follows a simple but powerful lifecycle:</p>

<ol>
<li><strong>Subscribe</strong> - Connect to one or more producers</li>
<li><strong>Demand</strong> - Ask for a specific number of events</li>
<li><strong>Receive</strong> - Get events from producers (never more than requested)</li>
<li><strong>Process</strong> - Handle each event</li>
<li><strong>Repeat</strong> - Ask for more events when ready</li>
</ol>

<p>The key insight is step 4: processing happens between demands. The consumer processes its current batch completely before asking for more. This creates natural backpressure - slow consumers automatically reduce the flow rate.</p>

<h2>Building Our First Consumer</h2>

<p>Let's build a consumer that processes the events from our producer. Create a new file at <code>lib/job_processor/consumer.ex</code>:</p>

<pre><code>defmodule JobProcessor.Consumer do
  use GenStage
  require Logger

  @doc """
  Starts the consumer.

  Like producers, consumers are just GenServer-like processes.
  The state can be anything you need for processing.
  """
  def start_link(opts \\ []) do
    GenStage.start_link(__MODULE__, opts)
  end

  @impl true
  def init(opts) do
    # The key difference: we declare ourselves as a :consumer
    # and specify which producer(s) to subscribe to
    {:consumer, opts, subscribe_to: [JobProcessor.Producer]}
  end

  @impl true
  def handle_events(events, _from, state) do
    Logger.info("Consumer received #{length(events)} events")

    # Process each event
    for event <- events do
      process_event(event, state)
    end

    # Always return {:noreply, [], state} for consumers
    # The empty list means we don't emit any events (we're not a producer)
    {:noreply, [], state}
  end

  defp process_event(event, state) do
    # For now, just log what we received
    Logger.info("Processing event: #{event}")
    IO.inspect({self(), event, state}, label: "Consumer processed")
  end
end</code></pre>

<h2>Understanding the Consumer Architecture</h2>

<p>Let's break down what makes this consumer work:</p>

<p><strong><code>use GenStage</code></strong> - Just like producers, consumers use the GenStage behavior. But the callbacks they implement are different.</p>

<p><strong><code>init/1</code> returns <code>{:consumer, state, options}</code></strong> - The crucial difference from producers. The first element declares this process as a consumer. The <code>subscribe_to</code> option tells GenStage which producers to connect to.</p>

<p><strong><code>handle_events/3</code> instead of <code>handle_demand/2</code></strong> - Consumers implement <code>handle_events/3</code>, which receives:</p>
<ul>
<li><code>events</code> - The list of events to process</li>
<li><code>from</code> - Which producer sent these events (usually ignored)</li>
<li><code>state</code> - The consumer's current state</li>
</ul>

<p><strong>The return value <code>{:noreply, [], state}</code></strong> - Consumers don't emit events (that's producers' job), so the events list is always empty. They just process and update their state.</p>

<h2>The Magic of Subscription</h2>

<p>Notice the <code>subscribe_to: [JobProcessor.Producer]</code> option. This does several important things:</p>

<p><strong>Automatic connection</strong> - GenStage handles finding and connecting to the producer. No manual process linking or monitoring.</p>

<p><strong>Automatic demand</strong> - The consumer automatically asks the producer for events. By default, it requests batches of up to 1000 events, but you can tune this.</p>

<p><strong>Fault tolerance</strong> - If the producer crashes and restarts, the consumer automatically reconnects. If the consumer crashes, it doesn't take down the producer.</p>

<p><strong>Flow control</strong> - The consumer won't receive more events than it asks for. If it's slow processing the current batch, no new events arrive until it's ready.</p>

<h2>Tuning Consumer Demand</h2>

<p>You can control how many events a consumer requests at once:</p>

<pre><code>def init(opts) do
  {:consumer, opts,
   subscribe_to: [
     {JobProcessor.Producer, min_demand: 5, max_demand: 50}
   ]}
end</code></pre>

<p><strong><code>min_demand</code></strong> - Don't ask for more events until we have fewer than this many</p>
<p><strong><code>max_demand</code></strong> - Never ask for more than this many events at once</p>

<p>This creates a buffering effect. The consumer will receive events in batches between min_demand and max_demand, giving you control over throughput vs. latency tradeoffs.</p>

<p>For job processing, you might want smaller batches to reduce memory usage:</p>

<pre><code>subscribe_to: [
  {JobProcessor.Producer, min_demand: 1, max_demand: 10}
]</code></pre>

<p>Or larger batches for higher throughput:</p>

<pre><code>subscribe_to: [
  {JobProcessor.Producer, min_demand: 100, max_demand: 1000}
]</code></pre>

<h2>Why This Design Matters</h2>

<p>The producer-consumer subscription model solves several classic distributed systems problems:</p>

<p><strong>Backpressure</strong> - Slow consumers naturally slow down the entire pipeline. No queues overflow, no memory explosions.</p>

<p><strong>Dynamic scaling</strong> - Add more consumers and they automatically start receiving events. Remove consumers and the remaining ones pick up the slack.</p>

<p><strong>Fault isolation</strong> - A crashing consumer doesn't affect others. A crashing producer can be restarted without losing in-flight work.</p>

<p><strong>Observable performance</strong> - You can see exactly where bottlenecks are by monitoring demand patterns. High accumulated demand = bottleneck downstream.</p>

<h2>Consumer Patterns</h2>

<p>Real-world consumers follow several common patterns:</p>

<p><strong>Database Writing Consumer:</strong></p>
<pre><code>def handle_events(events, _from, state) do
  # Batch insert for efficiency
  records = Enum.map(events, &transform_event/1)
  Repo.insert_all(MyTable, records)

  {:noreply, [], state}
end</code></pre>

<p><strong>HTTP API Consumer:</strong></p>
<pre><code>def handle_events(events, _from, state) do
  for event <- events do
    case HTTPoison.post(state.webhook_url, Jason.encode!(event)) do
      {:ok, %{status_code: 200}} -> :ok
      {:error, reason} -> Logger.error("Webhook failed: #{inspect(reason)}")
    end
  end

  {:noreply, [], state}
end</code></pre>

<p><strong>File Processing Consumer:</strong></p>
<pre><code>def handle_events(events, _from, state) do
  for event <- events do
    file_path = "/tmp/processed_#{event.id}.json"
    File.write!(file_path, Jason.encode!(event))
  end

  {:noreply, [], state}
end</code></pre>

<h2>Error Handling in Consumers</h2>

<p>What happens when event processing fails? In traditional queue systems, you need complex retry logic, dead letter queues, and careful state management.</p>

<p>With GenStage consumers, it's simpler. If a consumer crashes while processing events, those events are simply not acknowledged. When the consumer restarts, the producer still has them and will include them in the next batch.</p>

<p>For more sophisticated error handling, you can catch exceptions:</p>

<pre><code>def handle_events(events, _from, state) do
  for event <- events do
    try do
      process_event(event)
    rescue
      e ->
        Logger.error("Failed to process event #{event.id}: #{inspect(e)}")
        # Could send to dead letter queue, retry later, etc.
    end
  end

  {:noreply, [], state}
end</code></pre>

<p>But often, letting the process crash and restart is the right approach. It's simple, it clears any corrupted state, and the supervisor handles the restart automatically.</p>

<h1>Wiring It Together</h1>

<p>Now we have both pieces: a producer that emits events and a consumer that processes them. But they're just modules sitting in files. We need to start them as processes and connect them.</p>

<p>This is where OTP's supervision trees shine. We'll add both processes to our application's supervision tree, and OTP will ensure they start in the right order and restart if they crash.</p>

<p>Open <code>lib/job_processor/application.ex</code> and modify the <code>start/2</code> function:</p>

<pre><code>def start(_type, _args) do
  children = [
    # Start the Producer first
    JobProcessor.Producer,

    # Then start the Consumer
    # The consumer will automatically connect to the producer
    JobProcessor.Consumer,

    # Other children like Ecto, Phoenix endpoint, etc.
    JobProcessorWeb.Endpoint
  ]

  opts = [strategy: :one_for_one, name: JobProcessor.Supervisor]
  Supervisor.start_link(children, opts)
end</code></pre>

<p>That's it! The supervision tree will:</p>

<ol>
<li>Start the producer</li>
<li>Start the consumer</li>
<li>The consumer automatically subscribes to the producer</li>
<li>Events start flowing immediately</li>
</ol>

<h2>Why This Supervision Strategy Works</h2>

<p>The <code>:one_for_one</code> strategy means if one process crashes, only that process is restarted. This is perfect for our producer-consumer setup:</p>

<p><strong>Producer crashes</strong> - The consumer notices the connection is lost and waits. When the supervisor restarts the producer, the consumer automatically reconnects.</p>

<p><strong>Consumer crashes</strong> - The producer keeps running, just stops emitting events. When the supervisor restarts the consumer, it reconnects and processing resumes.</p>

<p>This is fault isolation in action. Problems in one part of the system don't cascade to other parts.</p>

<h2>Testing the Connection</h2>

<p>Let's see our producer and consumer working together. Start the application:</p>

<pre><code>mix phx.server</code></pre>

<p>You should see logs showing the consumer processing events from the producer. Each event will be displayed with the process ID, event number, and state - just like in our <code>process_event/2</code> function.</p>

<p>The beauty of this setup is its simplicity. No complex configuration, no manual connection management, no brittle polling loops. Just declare what you want (producer â†’ consumer) and GenStage handles the rest.</p>
`;

        const bobbyVersion = `
<h1>Writing A Job Runner (In Elixir)</h1>

<p>Applications must do work. This is typical of just about any program that reaches a sufficient size. In order to do that work, sometimes it's desirable to have it happen <em>elsewhere</em>. If you have built software, you have probably needed a background job.</p>

<p>In this situation, you are fundamentally using code to run other code. Erlang has a nice format for this, called the Erlang term format. It can store its data in a way it can be passed around and run by other nodes We are going to examine doing this in Elixir with "tools in the shed". We will have a single dependency called <code>gen_stage</code> that is built and maintained by the language's creator, Jose Valim.</p>

<p>For beginners, we will first cover a bit about Elixir and what it offers that might make this appealing</p>

<div class="aside collapsed">
<div class="aside-header" onclick="toggleAside(this.parentElement)">
    <span class="aside-label">Aside 1</span>
    <span class="aside-title">The Landscape of Job Processing</span>
</div>
<div class="aside-content">
<h2>The Landscape of Job Processing</h2>

<p>In Ruby, you might reach for Sidekiq. It's battle-tested, using Redis for storage and threads for concurrency. Jobs are JSON objects, workers pull from queues, and if something crashes, you hope your monitoring catches it. It works well until you need to scale beyond a single Redis instance or handle complex job dependencies.</p>

<p>Python developers often turn to Celery. It's more distributed by design, supporting multiple brokers and result backends. But the complexity shows - you're configuring RabbitMQ, dealing with serialization formats, and debugging issues across multiple moving parts. When a worker dies mid-job, recovery depends on how well you've configured acknowledgments and retries.</p>

<p>Go developers might use machinery or asynq, leveraging goroutines for concurrency. The static typing helps catch errors early, but you're still manually managing worker pools and carefully handling panics to prevent the whole process from dying.</p>

<p>Each solution reflects its language's strengths and limitations. They all converge on similar patterns: a persistent queue, worker processes, and lots of defensive programming. What if the language itself provided better primitives for this problem?</p>
</div>
</div>

<div class="aside collapsed">
<div class="aside-header" onclick="toggleAside(this.parentElement)">
    <span class="aside-label">Aside 2</span>
    <span class="aside-title">Why Elixir Works for Job Processing</span>
</div>
<div class="aside-content">
<h2>Why Elixir Works for Job Processing</h2>

<p><strong>Processes are the unit of concurrency.</strong> Not threads, not coroutines - processes. Each process has its own heap, runs concurrently, and can't corrupt another's memory. Starting one is measured in microseconds and takes about 2KB of memory. You don't manage a pool of workers; you spawn a process per job.</p>

<p><strong>Failure is isolated by default.</strong> When a process crashes, it dies alone. No corrupted global state, no locked mutexes, no zombie threads. The supervisor sees the death, logs it, and starts a fresh process. Your job processor doesn't need defensive try-catch blocks everywhere - it needs a good supervision tree.</p>

<p><strong>Message passing is the only way to communicate.</strong> No shared memory means no locks, no race conditions, no memory barriers. A process either receives a message or it doesn't. This constraint simplifies concurrent programming dramatically - you can reason about each process in isolation.</p>

<p><strong>The scheduler handles fairness.</strong> The BEAM VM runs its own scheduler, preemptively switching between processes every 2000 reductions. One process can't starve others by hogging the CPU. This is why Phoenix can handle millions of WebSocket connections - each connection is just another lightweight process.</p>

<p><strong>Distribution is built-in.</strong> Connect nodes with one function call. Send messages across the network with the same syntax as local messages. The Erlang Term Format serializes any data structure, including function references. Your job queue can span multiple machines without changing the core logic.</p>

<p><strong>Hot code reloading works.</strong> Deploy new code without stopping the system. The BEAM can run two versions of a module simultaneously, migrating processes gracefully. Your job processor can be upgraded while it's processing jobs.</p>

<p><strong>Introspection is exceptional.</strong> Connect to a running system and inspect any process. See its message queue, memory usage, current function. The observer GUI shows your entire system's health in real-time. When production misbehaves, you can debug it live.</p>

<p>These aren't features bolted on top - they're fundamental to how the BEAM VM works. When you build a job processor in Elixir, you're not fighting the language to achieve reliability and concurrency. You're using it as designed.</p>
</div>
</div>

<h1>Thinking About Job Runners, Producers, Consumers, and Events</h1>

<h2>The Architecture of Work</h2>

<p>At its core, a job runner is a meta concept. It is code that runs code. There will always be work to be done in any given system that has users. But ensuring work gets done when it cannot be handled in a blocking, synchronous matter (and you have the time to await results) is nearly impossible. The devil is in these details. How do you handle failure? What is our plan when we have a situation that could overwhelm our worker pool? We seek out answers to these questions as we do this dive.</p>

<p>GenStage answers the questions we have asked so far, in general, with demand driven architecture. Instead of pushing work out, workers pull when <em>they</em> are ready. This inversion becomes a very elegant abstraction in practice.</p>

<h2>Understanding Producer-Consumer Patterns</h2>

<p>The producer-consumer pattern isn't unique to Elixir. It's a fundamental pattern in distributed systems:</p>

<p><strong>In Apache Spark</strong>, RDDs (Resilient Distributed Datasets) flow through transformations. Each transformation is essentially a consumer of the previous stage and a producer for the next. Spark handles backpressure through its task scheduler - if executors are busy, new tasks wait.</p>

<p><strong>In Kafka Streams</strong>, topics act as buffers between producers and consumers. Consumers track their offset, pulling messages at their own pace. The broker handles persistence and replication.</p>

<p><strong>In Go channels</strong>, goroutines communicate through typed channels. A goroutine blocks when sending to a full channel or receiving from an empty one. This provides natural backpressure but requires careful capacity planning.</p>

<p>GenStage takes a different approach. There are no intermediate buffers or brokers. Producers and consumers negotiate directly:</p>

<ol>
<li>Consumer asks producer for work (specifying how much it can handle)</li>
<li>Producer responds with up to that many events</li>
<li>Consumer processes events and asks for more</li>
</ol>

<p>This creates a pull-based system with automatic flow control. No queues filling up, no brokers to manage, no capacity planning. The system self-regulates based on actual processing speed.</p>

<h2>What We're Actually Building</h2>

<p>Our job runner will have three core components:</p>

<p><strong>Producers</strong> - These generate or fetch work. In our case, they'll pull jobs from a database table. A producer doesn't decide who gets work - it simply responds to demand. When a consumer asks for 10 jobs, the producer queries the database for 10 unclaimed jobs and returns them.</p>

<p><strong>Consumers</strong> - These execute jobs. Each consumer is a separate Elixir process, isolated from others. When a consumer is ready for work, it asks its producer for events. After processing, it asks for more. If a consumer crashes while processing a job, only that job is affected.</p>

<p><strong>Events</strong> - The unit of work flowing through the system. In GenStage, everything is an event. For our job runner, an event is a job to be executed. Events flow from producers to consumers based on demand, never faster than consumers can handle.</p>

<h2>The Beauty of Modeling Everything as Events</h2>

<p>When you model work as events, powerful patterns emerge:</p>

<p><strong>Composition</strong> - You can chain stages together. A consumer can also be a producer for another stage. Want to add a step that enriches jobs before execution? Insert a producer-consumer between your current stages.</p>

<p><strong>Fan-out/Fan-in</strong> - One producer can feed multiple consumers (fan-out). Multiple producers can feed one consumer (fan-in). The demand mechanism ensures fair distribution.</p>

<p><strong>Buffering</strong> - Need a buffer? Add a producer-consumer that accumulates events before passing them on. The buffer only fills as fast as downstream consumers can drain it.</p>

<p><strong>Filtering</strong> - A producer-consumer can selectively forward events. Only want to process high-priority jobs? Filter them in a middle stage.</p>

<h2>Why This Matters for Job Processing</h2>

<p>Traditional job processors push jobs into queues. Workers poll these queues, hoping to grab work. This creates several problems:</p>

<ol>
<li><strong>Queue overflow</strong> - Producers can overwhelm the queue if consumers are slow</li>
<li><strong>Unfair distribution</strong> - Fast workers might grab all the work</li>
<li><strong>Visibility</strong> - Hard to see where bottlenecks are</li>
<li><strong>Error handling</strong> - What happens to in-flight jobs when a worker dies?</li>
</ol>

<p>GenStage's demand-driven model solves these elegantly:</p>

<ol>
<li><strong>No overflow</strong> - Producers only generate what's demanded</li>
<li><strong>Fair distribution</strong> - Each consumer gets what it asks for</li>
<li><strong>Clear bottlenecks</strong> - Slow stages naturally build up demand</li>
<li><strong>Clean errors</strong> - Crashed consumers simply stop demanding; their work remains unclaimed</li>
</ol>

<p>This isn't theoretical. Telecom systems have used these patterns for decades. When you make a phone call, switches don't push calls through the network - each hop pulls when ready. This prevents network overload even during disasters when everyone tries to call at once.</p>

<p>We're applying the same battle-tested patterns to job processing. The result is a system that's naturally resilient, self-balancing, and surprisingly simple to reason about.</p>

<p>Ready to see how this translates to code? Let's build our first producer.</p>

<h1>Building the Foundation</h1>

<h2>Step 1: Creating Your Phoenix Project</h2>

<p>Let's start fresh with a new Phoenix project. Open your terminal and run:</p>

<pre><code>mix phx.new job_processor --no-dashboard --no-mailer
cd job_processor</code></pre>

<p>We're keeping it lean - no dashboard or mailer for now. When prompted to install dependencies, say yes.</p>

<p>Why Phoenix? We're not building a web app, but Phoenix gives us:</p>
<ul>
<li>A supervision tree already set up</li>
<li>Configuration management</li>
<li>A database connection (Ecto)</li>
<li>LiveView for our monitoring dashboard (later)</li>
</ul>

<p>Think of Phoenix as our application framework, not just a web framework.</p>

<h2>Step 2: Adding GenStage</h2>

<p>Open <code>mix.exs</code> and add GenStage to your dependencies:</p>

<pre><code>defp deps do
  [
    {:phoenix, "~> 1.7.12"},
    {:phoenix_ecto, "~> 4.5"},
    {:ecto_sql, "~> 3.11"},
    {:postgrex, ">= 0.0.0"},
    {:phoenix_html, "~> 4.1"},
    {:phoenix_live_reload, "~> 1.2", only: :dev},
    {:phoenix_live_view, "~> 0.20.14"},
    {:telemetry_metrics, "~> 0.6"},
    {:telemetry_poller, "~> 1.0"},
    {:jason, "~> 1.2"},
    {:bandit, "~> 1.2"},

    # Add this line
    {:gen_stage, "~> 1.2"}
  ]
end</code></pre>

<p>Now fetch the dependency:</p>

<pre><code>mix deps.get</code></pre>

<p>That's it. One dependency. GenStage is maintained by the Elixir core team, so it follows the same design principles as the language itself.</p>

<h2>Step 3: Understanding GenStage's Mental Model</h2>

<p>Before we write code, let's cement the mental model. GenStage orchestrates three types of processes:</p>

<p><strong>Producers</strong> emit events. They don't push events anywhere - they hold them until a consumer asks. Think of a producer as a lazy river of data. The water (events) only flows when someone downstream opens a valve (demands).</p>

<p><strong>Consumers</strong> receive events. They explicitly ask producers for a specific number of events. This is the key insight: consumers control the flow rate, not producers.</p>

<p><strong>Producer-Consumers</strong> do both. They receive events from upstream, transform them, and emit to downstream. Perfect for building pipelines.</p>

<p>Every GenStage process follows this lifecycle:</p>
<ol>
<li>Start and connect to other stages</li>
<li>Consumer sends demand upstream</li>
<li>Producer receives demand and emits events</li>
<li>Consumer receives and processes events</li>
<li>Repeat from step 2</li>
</ol>

<p>The demand mechanism is what makes this special. In a traditional queue, you might have:</p>

<pre><code># Traditional approach - producer decides when to push
loop do
  job = create_job()
  Queue.push(job)  # What if queue is full?
end</code></pre>

<p>With GenStage:</p>

<pre><code># GenStage approach - consumer decides when to pull
def handle_demand(demand, state) do
  jobs = create_jobs(demand)  # Only create what's asked for
  {:noreply, jobs, state}
end</code></pre>

<p>The consumer is in control. It's impossible to overwhelm a consumer because it only gets what it asked for.</p>

<h2>Step 4: Creating the Producer</h2>

<p>Now for the meat of it. Let's build a producer that understands our job processing needs. Create a new file at <code>lib/job_processor/producer.ex</code>:</p>

<pre><code>defmodule JobProcessor.Producer do
  use GenStage
  require Logger

  @doc """
  Starts the producer with an initial state.

  The state can be anything, but we'll use a counter to start simple.
  """
  def start_link(initial \\ 0) do
    GenStage.start_link(__MODULE__, initial, name: __MODULE__)
  end

  @impl true
  def init(counter) do
    Logger.info("Producer starting with counter: #{counter}")
    {:producer, counter}
  end

  @impl true
  def handle_demand(demand, state) do
    Logger.info("Producer received demand for #{demand} events")

    # Generate events to fulfill demand
    events = Enum.to_list(state..(state + demand - 1))

    # Update our state
    new_state = state + demand

    # Return events and new state
    {:noreply, events, new_state}
  end
end</code></pre>

<p>Let's dissect this line by line:</p>

<p><strong><code>use GenStage</code></strong> - This macro brings in the GenStage behavior. It's like <code>use GenServer</code> but for stages. It requires us to implement certain callbacks.</p>

<p><strong><code>start_link/1</code></strong> - Standard OTP pattern. We name the process after its module so we can find it easily. In production, you might want multiple producers, so you'd make the name configurable.</p>

<p><strong><code>init/1</code></strong> - The crucial part: <code>{:producer, counter}</code>. The first element declares this as a producer. The second is our initial state. GenStage now knows this process will emit events when asked.</p>

<p><strong><code>handle_demand/2</code></strong> - The heart of a producer. This callback fires when consumers ask for events. The arguments are:</p>
<ul>
<li><code>demand</code> - How many events the consumer wants</li>
<li><code>state</code> - Our current state</li>
</ul>

<p>The return value <code>{:noreply, events, new_state}</code> means:</p>
<ul>
<li><code>:noreply</code> - We're responding to demand, not a synchronous call</li>
<li><code>events</code> - The list of events to emit (must be a list)</li>
<li><code>new_state</code> - Our updated state</li>
</ul>

<h3>The Demand Buffer</h3>

<p>Here's something subtle but important: GenStage maintains an internal demand buffer. If multiple consumers ask for events before you can fulfill them, GenStage aggregates the demand.</p>

<p>For example:</p>
<ol>
<li>Consumer A asks for 10 events</li>
<li>Consumer B asks for 5 events</li>
<li>Your <code>handle_demand/2</code> receives demand for 15 events</li>
</ol>

<p>This batching is efficient and prevents your producer from being called repeatedly for small demands.</p>

<h3>What if You Can't Fulfill Demand?</h3>

<p>Sometimes you can't produce as many events as demanded. That's fine:</p>

<pre><code>def handle_demand(demand, state) do
  available = calculate_available_work()

  if available >= demand do
    events = fetch_events(demand)
    {:noreply, events, state}
  else
    # Can only partially fulfill demand
    events = fetch_events(available)
    {:noreply, events, state}
  end
end</code></pre>

<p>GenStage tracks unfulfilled demand. If you return fewer events than demanded, it remembers. The next time you have events available, you can emit them even without new demand:</p>

<pre><code>def handle_info(:new_data_available, state) do
  events = fetch_available_events()
  {:noreply, events, state}
end</code></pre>

<h3>Producer Patterns</h3>

<p>Our simple counter producer is just the beginning. Real-world producers follow several patterns:</p>

<p><strong>Database Polling Producer:</strong></p>
<pre><code>def handle_demand(demand, state) do
  jobs = Repo.all(
    from j in Job,
    where: j.status == "pending",
    limit: ^demand,
    lock: "FOR UPDATE SKIP LOCKED"
  )

  job_ids = Enum.map(jobs, & &1.id)

  Repo.update_all(
    from(j in Job, where: j.id in ^job_ids),
    set: [status: "processing"]
  )

  {:noreply, jobs, state}
end</code></pre>

<p><strong>Rate-Limited Producer:</strong></p>
<pre><code>def handle_demand(demand, %{rate_limit: limit} = state) do
  now = System.monotonic_time(:millisecond)
  time_passed = now - state.last_emit

  allowed = min(demand, div(time_passed * limit, 1000))

  if allowed > 0 do
    events = generate_events(allowed)
    {:noreply, events, %{state | last_emit: now}}
  else
    # Schedule retry
    Process.send_after(self(), :retry_demand, 100)
    {:noreply, [], state}
  end
end</code></pre>

<p><strong>Buffering Producer:</strong></p>
<pre><code>def handle_demand(demand, %{buffer: buffer} = state) do
  {to_emit, remaining} = Enum.split(buffer, demand)

  if length(to_emit) < demand do
    # Buffer exhausted, try to refill
    new_events = fetch_more_events()
    all_events = to_emit ++ new_events
    {to_emit_now, to_buffer} = Enum.split(all_events, demand)
    {:noreply, to_emit_now, %{state | buffer: to_buffer}}
  else
    {:noreply, to_emit, %{state | buffer: remaining}}
  end
end</code></pre>

<h3>Testing Your Producer</h3>

<p>Let's make sure our producer works. Create <code>test/job_processor/producer_test.exs</code>:</p>

<pre><code>defmodule JobProcessor.ProducerTest do
  use ExUnit.Case
  alias JobProcessor.Producer

  test "producer emits events on demand" do
    {:ok, producer} = Producer.start_link(0)

    # Manually subscribe and ask for events
    {:ok, _subscription} = GenStage.sync_subscribe(self(), to: producer, max_demand: 5)

    # We should receive 5 events (0 through 4)
    assert_receive {:"$gen_consumer", {_, _}, [0, 1, 2, 3, 4]}
  end

  test "producer maintains state across demands" do
    {:ok, producer} = Producer.start_link(10)

    # First demand
    {:ok, _} = GenStage.sync_subscribe(self(), to: producer, max_demand: 3)
    assert_receive {:"$gen_consumer", {_, _}, [10, 11, 12]}

    # Second demand should continue from where we left off
    send(producer, {:"$gen_producer", {self(), nil}, {:ask, 2}})
    assert_receive {:"$gen_consumer", {_, _}, [13, 14]}
  end
end</code></pre>

<p>Run the tests with <code>mix test</code>.</p>

<h3>The Power of Stateful Producers</h3>

<p>Our producer maintains state - a simple counter. But state can be anything:</p>

<ul>
<li>A database connection for polling</li>
<li>A buffer of pre-fetched events</li>
<li>Rate limiting information</li>
<li>Metrics and telemetry data</li>
</ul>

<p>Because each producer is just an Erlang process, it's isolated. If one producer crashes, others continue. The supervisor restarts the crashed producer with a fresh state.</p>

<p>This is different from thread-based systems where shared state requires locks. Each producer owns its state exclusively. No locks, no race conditions, no defensive programming.</p>

<h3>What We've Built</h3>

<p>Our producer is deceptively simple, but it demonstrates core principles:</p>

<ol>
<li><strong>Demand-driven</strong> - Only produces when asked</li>
<li><strong>Stateful</strong> - Maintains its own isolated state</li>
<li><strong>Supervised</strong> - Can crash and restart safely</li>
<li><strong>Testable</strong> - Easy to verify behavior</li>
</ol>

<p>In the next section, we'll build consumers that process these events. But the producer is the foundation - it controls the flow of work through our system.</p>

<p>In the next section, we'll build consumers that process these events. But the producer is the foundation - it controls the flow of work through our system.</p>

<h1>Building A Consumer</h1>

<p>Now that we have a producer emitting events, we need something to consume them. This is where consumers come in - they're the workers that actually process the events flowing through our system.</p>

<p>But here's the beautiful thing about GenStage consumers: they're not passive recipients waiting for work to be thrown at them. They're active participants in the flow control. A consumer decides how much work it can handle and explicitly asks for that amount. No more, no less.</p>

<p>Think about how this changes the dynamics. In a traditional message queue, producers blast messages into a queue, hoping consumers can keep up. If consumers fall behind, the queue grows. If consumers are faster than expected, they sit idle waiting for work. It's a constant balancing act with lots of manual tuning.</p>

<p>GenStage flips this completely. Consumers know their own capacity better than anyone else. They know if they're currently processing a heavy job, if they're running low on memory, or if they're about to restart. So they ask for exactly what they can handle right now.</p>

<h2>The Consumer's Lifecycle</h2>

<p>A GenStage consumer follows a simple but powerful lifecycle:</p>

<ol>
<li><strong>Subscribe</strong> - Connect to one or more producers</li>
<li><strong>Demand</strong> - Ask for a specific number of events</li>
<li><strong>Receive</strong> - Get events from producers (never more than requested)</li>
<li><strong>Process</strong> - Handle each event</li>
<li><strong>Repeat</strong> - Ask for more events when ready</li>
</ol>

<p>The key insight is step 4: processing happens between demands. The consumer processes its current batch completely before asking for more. This creates natural backpressure - slow consumers automatically reduce the flow rate.</p>

<h2>Building Our First Consumer</h2>

<p>Let's build a consumer that processes the events from our producer. Create a new file at <code>lib/job_processor/consumer.ex</code>:</p>

<pre><code>defmodule JobProcessor.Consumer do
  use GenStage
  require Logger

  @doc """
  Starts the consumer.

  Like producers, consumers are just GenServer-like processes.
  The state can be anything you need for processing.
  """
  def start_link(opts \\ []) do
    GenStage.start_link(__MODULE__, opts)
  end

  @impl true
  def init(opts) do
    # The key difference: we declare ourselves as a :consumer
    # and specify which producer(s) to subscribe to
    {:consumer, opts, subscribe_to: [JobProcessor.Producer]}
  end

  @impl true
  def handle_events(events, _from, state) do
    Logger.info("Consumer received #{length(events)} events")

    # Process each event
    for event <- events do
      process_event(event, state)
    end

    # Always return {:noreply, [], state} for consumers
    # The empty list means we don't emit any events (we're not a producer)
    {:noreply, [], state}
  end

  defp process_event(event, state) do
    # For now, just log what we received
    Logger.info("Processing event: #{event}")
    IO.inspect({self(), event, state}, label: "Consumer processed")
  end
end</code></pre>

<h2>Understanding the Consumer Architecture</h2>

<p>Let's break down what makes this consumer work:</p>

<p><strong><code>use GenStage</code></strong> - Just like producers, consumers use the GenStage behavior. But the callbacks they implement are different.</p>

<p><strong><code>init/1</code> returns <code>{:consumer, state, options}</code></strong> - The crucial difference from producers. The first element declares this process as a consumer. The <code>subscribe_to</code> option tells GenStage which producers to connect to.</p>

<p><strong><code>handle_events/3</code> instead of <code>handle_demand/2</code></strong> - Consumers implement <code>handle_events/3</code>, which receives:</p>
<ul>
<li><code>events</code> - The list of events to process</li>
<li><code>from</code> - Which producer sent these events (usually ignored)</li>
<li><code>state</code> - The consumer's current state</li>
</ul>

<p><strong>The return value <code>{:noreply, [], state}</code></strong> - Consumers don't emit events (that's producers' job), so the events list is always empty. They just process and update their state.</p>

<h2>The Magic of Subscription</h2>

<p>Notice the <code>subscribe_to: [JobProcessor.Producer]</code> option. This does several important things:</p>

<p><strong>Automatic connection</strong> - GenStage handles finding and connecting to the producer. No manual process linking or monitoring.</p>

<p><strong>Automatic demand</strong> - The consumer automatically asks the producer for events. By default, it requests batches of up to 1000 events, but you can tune this.</p>

<p><strong>Fault tolerance</strong> - If the producer crashes and restarts, the consumer automatically reconnects. If the consumer crashes, it doesn't take down the producer.</p>

<p><strong>Flow control</strong> - The consumer won't receive more events than it asks for. If it's slow processing the current batch, no new events arrive until it's ready.</p>

<h2>Tuning Consumer Demand</h2>

<p>You can control how many events a consumer requests at once:</p>

<pre><code>def init(opts) do
  {:consumer, opts,
   subscribe_to: [
     {JobProcessor.Producer, min_demand: 5, max_demand: 50}
   ]}
end</code></pre>

<p><strong><code>min_demand</code></strong> - Don't ask for more events until we have fewer than this many</p>
<p><strong><code>max_demand</code></strong> - Never ask for more than this many events at once</p>

<p>This creates a buffering effect. The consumer will receive events in batches between min_demand and max_demand, giving you control over throughput vs. latency tradeoffs.</p>

<p>For job processing, you might want smaller batches to reduce memory usage:</p>

<pre><code>subscribe_to: [
  {JobProcessor.Producer, min_demand: 1, max_demand: 10}
]</code></pre>

<p>Or larger batches for higher throughput:</p>

<pre><code>subscribe_to: [
  {JobProcessor.Producer, min_demand: 100, max_demand: 1000}
]</code></pre>

<h2>Why This Design Matters</h2>

<p>The producer-consumer subscription model solves several classic distributed systems problems:</p>

<p><strong>Backpressure</strong> - Slow consumers naturally slow down the entire pipeline. No queues overflow, no memory explosions.</p>

<p><strong>Dynamic scaling</strong> - Add more consumers and they automatically start receiving events. Remove consumers and the remaining ones pick up the slack.</p>

<p><strong>Fault isolation</strong> - A crashing consumer doesn't affect others. A crashing producer can be restarted without losing in-flight work.</p>

<p><strong>Observable performance</strong> - You can see exactly where bottlenecks are by monitoring demand patterns. High accumulated demand = bottleneck downstream.</p>

<h2>Consumer Patterns</h2>

<p>Real-world consumers follow several common patterns:</p>

<p><strong>Database Writing Consumer:</strong></p>
<pre><code>def handle_events(events, _from, state) do
  # Batch insert for efficiency
  records = Enum.map(events, &transform_event/1)
  Repo.insert_all(MyTable, records)

  {:noreply, [], state}
end</code></pre>

<p><strong>HTTP API Consumer:</strong></p>
<pre><code>def handle_events(events, _from, state) do
  for event <- events do
    case HTTPoison.post(state.webhook_url, Jason.encode!(event)) do
      {:ok, %{status_code: 200}} -> :ok
      {:error, reason} -> Logger.error("Webhook failed: #{inspect(reason)}")
    end
  end

  {:noreply, [], state}
end</code></pre>

<p><strong>File Processing Consumer:</strong></p>
<pre><code>def handle_events(events, _from, state) do
  for event <- events do
    file_path = "/tmp/processed_#{event.id}.json"
    File.write!(file_path, Jason.encode!(event))
  end

  {:noreply, [], state}
end</code></pre>

<h2>Error Handling in Consumers</h2>

<p>What happens when event processing fails? In traditional queue systems, you need complex retry logic, dead letter queues, and careful state management.</p>

<p>With GenStage consumers, it's simpler. If a consumer crashes while processing events, those events are simply not acknowledged. When the consumer restarts, the producer still has them and will include them in the next batch.</p>

<p>For more sophisticated error handling, you can catch exceptions:</p>

<pre><code>def handle_events(events, _from, state) do
  for event <- events do
    try do
      process_event(event)
    rescue
      e ->
        Logger.error("Failed to process event #{event.id}: #{inspect(e)}")
        # Could send to dead letter queue, retry later, etc.
    end
  end

  {:noreply, [], state}
end</code></pre>

<p>But often, letting the process crash and restart is the right approach. It's simple, it clears any corrupted state, and the supervisor handles the restart automatically.</p>

<h1>Wiring It Together</h1>

<p>Now we have both pieces: a producer that emits events and a consumer that processes them. But they're just modules sitting in files. We need to start them as processes and connect them.</p>

<p>This is where OTP's supervision trees shine. We'll add both processes to our application's supervision tree, and OTP will ensure they start in the right order and restart if they crash.</p>

<p>Open <code>lib/job_processor/application.ex</code> and modify the <code>start/2</code> function:</p>

<pre><code>def start(_type, _args) do
  children = [
    # Start the Producer first
    JobProcessor.Producer,

    # Then start the Consumer
    # The consumer will automatically connect to the producer
    JobProcessor.Consumer,

    # Other children like Ecto, Phoenix endpoint, etc.
    JobProcessorWeb.Endpoint
  ]

  opts = [strategy: :one_for_one, name: JobProcessor.Supervisor]
  Supervisor.start_link(children, opts)
end</code></pre>

<p>That's it! The supervision tree will:</p>

<ol>
<li>Start the producer</li>
<li>Start the consumer</li>
<li>The consumer automatically subscribes to the producer</li>
<li>Events start flowing immediately</li>
</ol>

<h2>Why This Supervision Strategy Works</h2>

<p>The <code>:one_for_one</code> strategy means if one process crashes, only that process is restarted. This is perfect for our producer-consumer setup:</p>

<p><strong>Producer crashes</strong> - The consumer notices the connection is lost and waits. When the supervisor restarts the producer, the consumer automatically reconnects.</p>

<p><strong>Consumer crashes</strong> - The producer keeps running, just stops emitting events. When the supervisor restarts the consumer, it reconnects and processing resumes.</p>

<p>This is fault isolation in action. Problems in one part of the system don't cascade to other parts.</p>

<h2>Testing the Connection</h2>

<p>Let's see our producer and consumer working together. Start the application:</p>

<pre><code>mix phx.server</code></pre>

<p>You should see logs</p>
`;

        let currentView = 'original';
        let isSideBySide = false;

        function toggleAside(asideElement) {
            asideElement.classList.toggle('collapsed');
        }

        function showVersion(version) {
            // If we're in side-by-side mode, switch back to single view first
            if (isSideBySide) {
                isSideBySide = false;
                document.getElementById('sideBySideBtn').classList.remove('active');
                document.getElementById('singleView').classList.remove('hidden');
                document.getElementById('sideBySideView').classList.add('hidden');
            }

            currentView = version;
            document.getElementById('originalBtn').classList.toggle('active', version === 'original');
            document.getElementById('bobbyBtn').classList.toggle('active', version === 'bobby');

            let content, labelText, labelClass;
            if (version === 'original') {
                content = originalVersion;
                labelText = '2015 ORIGINAL';
                labelClass = 'version-label original';
            } else {
                content = bobbyVersion;
                labelText = 'BOBBY VERSION';
                labelClass = 'version-label bobby';
            }

            document.getElementById('singleContent').innerHTML = content;

            const label = document.querySelector('#singleView .version-label');
            label.textContent = labelText;
            label.className = labelClass;
        }

        function toggleSideBySide() {
            isSideBySide = !isSideBySide;
            document.getElementById('sideBySideBtn').classList.toggle('active', isSideBySide);

            if (isSideBySide) {
                document.getElementById('singleView').classList.add('hidden');
                document.getElementById('sideBySideView').classList.remove('hidden');
                document.getElementById('originalContent').innerHTML = originalVersion;
                document.getElementById('bobbyContent').innerHTML = bobbyVersion;
            } else {
                document.getElementById('singleView').classList.remove('hidden');
                document.getElementById('sideBySideView').classList.add('hidden');
                showVersion(currentView);
            }
        }

        // Initialize
        showVersion('original');
    </script>
</body>
</html>